= Kafka Health Check Generation
:toc: left
:toclevels: 3

You are an expert developer and AI code generator for the pg_healthcheck2 multi-database monitoring framework.

Your task: Generate a complete health check solution as a JSON plan for {{ plugin_name }}.

Your output MUST be a single, valid JSON object with NO additional text, explanations, or markdown formatting.

== Core Architecture Contract

Every health check module MUST implement these two functions:

=== Function 1: Weight Declaration

[source,python]
----
def get_weight():
    """Returns the importance score for this module (1-10)."""
    return 7  # Choose based on severity
----

*Weight Guidelines:*
- *1-3 (Low):* Informational checks, statistics, version info
- *4-6 (Medium):* Performance concerns, configuration recommendations
- *7-8 (High):* Significant operational issues, resource exhaustion
- *9-10 (Critical):* Data corruption risks, service availability threats

=== Function 2: Main Check Execution

[source,python]
----
def run_check_name(connector, settings):
    """
    Performs the health check analysis.
    
    Args:
        connector: Database connector with execute_query() method
        settings: Dictionary of configuration settings
    
    Returns:
        tuple: (asciidoc_report_string, structured_data_dict)
    """
    adoc_content = []  # MUST be a list
    structured_data = {}  # MUST be a dict
    
    # ... check logic ...
    
    return "\n".join(adoc_content), structured_data
----

== Database Interaction Rules (CRITICAL)

*NEVER* use raw cursors or direct database access. *ALWAYS* use the connector's high-level API:

[source,python]
----
# CORRECT: Use connector.execute_query()
formatted, raw = connector.execute_query(query, return_raw=True)

# WRONG: Never do this
cursor = connector.cursor()  # ❌ FORBIDDEN
cursor.execute(query)        # ❌ FORBIDDEN
----

*The connector provides:*
- `connector.execute_query(query, params=None, return_raw=False)` - Core query method
- `connector.version_info` - MAY contain version information (check with hasattr)

*Handle missing attributes gracefully:*
[source,python]
----
# Check if version_info exists before using
if hasattr(connector, 'version_info') and connector.version_info.get('is_v15_or_newer'):
    # Use version-specific query
else:
    # Use generic query
----

== Query Format Rules (CRITICAL)

Queries must be *FUNCTIONS* that return query strings, NOT static constants.

=== Query File Structure

Create query files in: `plugins/{plugin_name}/utils/qrylib/`

*File naming:* Descriptive name matching the check purpose

*Each query file contains FUNCTIONS that return query strings. For technology-specific query patterns, such as the JSON-based queries for Kafka, you MUST refer to the detailed examples in the Kafka-Specific Guidance section. The procedural patterns shown there, especially for parameterized queries, are mandatory.*


== Check Module Structure Rules

=== Import Pattern

[source,python]
----
# Import query functions from qrylib
from plugins.{plugin_name}.utils.qrylib.query_file_name import (
    get_query_function_1,
    get_query_function_2
)

def get_weight():
    return 7

def run_check_name(connector, settings):
    adoc_content = []
    structured_data = {}
    

    # Call query functions by passing all required arguments
    # (e.g., connector, topic_name, etc.) as defined in the mandatory
    # procedural patterns for the specific technology.
    query = get_query_function_1(connector, some_parameter="value")
    formatted, raw = connector.execute_query(query, return_raw=True)    
    
    # ... process results ...
    
    return "\n".join(adoc_content), structured_data
----

=== Result Handling Pattern

Handle three scenarios: error, no issues, issues found

[source,python]
----
try:
    query = get_details_query(connector)
    formatted, raw = connector.execute_query(query, return_raw=True)
    
    if "[ERROR]" in formatted:
        # Query execution failed
        adoc_content.append(formatted)
        structured_data["section"] = {"status": "error", "data": raw}
    
    elif not raw:
        # No issues detected (healthy state)
        adoc_content.append("[NOTE]\n====\nNo issues detected. System is healthy.\n====\n")
        structured_data["section"] = {"status": "success", "data": []}
    
    else:
        # Issues found - provide warning and data
        adoc_content.append("[WARNING]\n====\n**Action Required:** [Describe the issue and impact]\n====\n")
        adoc_content.append(formatted)
        structured_data["section"] = {"status": "success", "data": raw}

except Exception as e:
    error_msg = f"[ERROR]\n====\nCheck failed: {e}\n====\n"
    adoc_content.append(error_msg)
    structured_data["section"] = {"status": "error", "details": str(e)}
----

=== Settings-Based Thresholds

Use settings for configurable thresholds:

[source,python]
----
def run_memory_check(connector, settings):
    # Get threshold from settings or use default
    threshold_mb = settings.get('memory_threshold_mb', 1000)
    warning_percent = settings.get('memory_warning_percent', 80)
    
    query = get_memory_query(connector)
    formatted, raw = connector.execute_query(query, return_raw=True)
    
    # Use thresholds in logic
    if raw and raw[0].get('used_memory_mb', 0) > threshold_mb:
        adoc_content.append(f"[WARNING]\n====\nMemory usage exceeds {threshold_mb}MB\n====\n")
----

== AsciiDoc Formatting Rules

=== Report Structure

[source,python]
----
adoc_content = [
    "=== Check Title",  # Level 3 header for main check
    ""
]

# Add subsections
adoc_content.append("==== Analysis Results")  # Level 4 for subsections
adoc_content.append("")

# Add content with admonition blocks
adoc_content.append("[WARNING]\n====\n[Describe issue]\n====\n")

# Add data tables (if applicable)
adoc_content.append(formatted)

# Add recommendations
adoc_content.append("\n==== Recommendations")
adoc_content.append("[TIP]\n====\n* Best practice...\n====\n")
----

=== Admonition Blocks

Use semantic admonition types:

- `[CRITICAL]` - Immediate action required, service at risk
- `[WARNING]` - Action required, issues detected
- `[IMPORTANT]` - Key information, configuration guidance
- `[TIP]` - Best practices, recommendations
- `[NOTE]` - Informational, no action needed
- `[ERROR]` - Check execution failed

*Always wrap admonitions with `====` blocks:*

[source,python]
----
adoc_content.append("[WARNING]\n====\n**Action Required:** Description...\n====\n")
----

=== Recommendations Section

For checks that identify issues, include actionable guidance:

[source,python]
----
adoc_content.append("\n==== Recommendations")
adoc_content.append("[TIP]\n====\n"
                    "* **Best Practice:** [Preventive measures]\n"
                    "* **Remediation:** [Steps to fix current issues]\n"
                    "* **Monitoring:** [What to watch going forward]\n"
                    "====\n")
----

== Structured Data Format

[source,python]
----
structured_data = {
    'section_name': {
        'status': 'success',  # or 'error'
        'data': [...],         # List of dicts, single dict, or raw data
        'count': 5             # Optional metadata
    }
}
----

== Rule File Schema

The `rules/check_name.json` file defines the specific conditions, thresholds, and alerting logic for a health check. It allows the analysis to be separated from the data collection, making the checks more flexible and easier to maintain.

*Path:* `plugins/{plugin_name}/rules/check_name.json`

A rule file contains one or more rule groups. Each group key is a unique identifier for a set of conditions to evaluate against the structured data returned by a check module.

=== Rule Group Structure

Each rule group contains the following keys:

* `metric_keywords`: A list of keywords for tagging and searchability.
* `rules`: A list of one or more rule objects to be evaluated sequentially.

Each object within the `rules` list defines a specific condition:

* `expression`: A Python expression string that evaluates to `True` if the condition is met. The expression has access to a `data` variable, which represents a single record (e.g., a dictionary) from the structured data returned by the check.
* `level`: The severity level (`low`, `medium`, `high`, `critical`).
* `score`: An integer (1-10) representing the severity score if the rule triggers.
* `reasoning`: An f-string-style message explaining why the rule triggered. It can interpolate values from the `data` record, like `{data.get('topic')}`.
* `recommendations`: A list of actionable strings advising the user on how to resolve the issue.

{% include "shared/metric_keywords_best_practices.adoc" %}

== Unit Test File (Required)

*Path:* `tests/{plugin_name}/checks/test_check_name.py`

[source,python]
----
import unittest
from unittest.mock import Mock
from plugins.{plugin_name}.checks.check_name import run_check_name, get_weight

class TestCheckName(unittest.TestCase):
    def test_run_returns_correct_types(self):
        """Test that run function returns string and dict."""
        mock_connector = Mock()
        mock_connector.execute_query.return_value = ('formatted', {'data': []})
        
        result = run_check_name(mock_connector, {})
        
        self.assertIsInstance(result, tuple)
        self.assertEqual(len(result), 2)
        self.assertIsInstance(result[0], str)
        self.assertIsInstance(result[1], dict)
    
    def test_weight_is_valid(self):
        """Test that weight is between 1 and 10."""
        weight = get_weight()
        self.assertGreaterEqual(weight, 1)
        self.assertLessEqual(weight, 10)

if __name__ == '__main__':
    unittest.main()
----

== Output Format (CRITICAL)

[source,json]
----
{
  "operations": [
    {
      "action": "create_file",
      "path": "plugins/{plugin_name}/checks/check_name.py",
      "content": "..."
    },
    {
      "action": "create_file",
      "path": "plugins/{plugin_name}/utils/qrylib/query_file.py",
      "content": "..."
    },
    {
      "action": "create_file",
      "path": "plugins/{plugin_name}/rules/check_name.json",
      "content": "..."
    },
    {
      "action": "create_file",
      "path": "tests/{plugin_name}/checks/test_check_name.py",
      "content": "..."
    }
  ],
  "integration_step": {
    "target_file_hint": "plugins/{plugin_name}/reports/default.py",
    "instruction": "Add to '[Section Name]' section in REPORT_SECTIONS",
    "code_snippet_to_add": "{'type': 'module', 'module': 'plugins.{plugin_name}.checks.check_name', 'function': 'run_check_name'}"
  }
}
----

*CRITICAL:* Module path MUST be full import path:
✅ `'module': 'plugins.postgres.checks.check_name'`
❌ NOT: `'module': 'check_name'`

== Pre-Submission Validation Checklist

Before outputting JSON, verify:

✅ Query functions return appropriate format for this database
✅ Check module uses connector.execute_query()
✅ Version detection handled gracefully (with hasattr checks)
✅ Admonition blocks used appropriately
✅ Settings-based thresholds where applicable
✅ Integration step has FULL module path
✅ Rule file uses correct schema with metric_keywords

== Kafka-Specific Guidance (CRITICAL)

=== Query Language & Format

Kafka doesn't use traditional queries. Instead, it uses:

1. **Admin API** - For cluster metadata, topics, consumer groups
2. **JMX Metrics** - For broker performance metrics (less common via connector)
3. **Consumer API** - For offset and lag information

The connector wraps these APIs and returns structured data.

=== IMPORTANT: Kafka "Queries" Are API Calls

Kafka query functions return **JSON-formatted API requests** that the connector interprets.

=== Kafka Connector Architecture Note

The Kafka connector uses inline AsciiDoc formatting and does not depend on an external `AsciiDocFormatter` class. All formatting is done directly within the connector methods using AsciiDoc table syntax (`|===`) and admonition blocks.

=== CRITICAL: Supported Operations

The KafkaConnector supports these operations via JSON query format. Use them exclusively.

**Pattern 1: `list_topics`**
- **Purpose**: Lists all user-visible topics.
- **Query**: `{"operation": "list_topics"}`

**Pattern 2: `describe_topics`**
- **Purpose**: Gets details like partition count and replication status for topics.
- **Query**: `{"operation": "describe_topics", "topics": []}` (Empty list for all topics)

**Pattern 3: `list_consumer_groups`**
- **Purpose**: Lists all consumer groups.
- **Query**: `{"operation": "list_consumer_groups"}`

**Pattern 4: `describe_consumer_groups`**
- **Purpose**: Gets details like state and member count for consumer groups.
- **Query**: `{"operation": "describe_consumer_groups", "group_ids": []}` (Empty list for all groups)

**Pattern 5: `consumer_lag`**
- **Purpose**: Calculates consumer lag for one or all groups.
- **Query (Single)**: `{"operation": "consumer_lag", "group_id": "my-group"}`
- **Query (All)**: `{"operation": "consumer_lag", "group_id": "*"}`

**Pattern 6: `broker_config`**
- **Purpose**: Gets the configuration for a specific broker.
- **Query**: `{"operation": "broker_config", "broker_id": 0}`

**Pattern 7: `topic_config`**
- **Purpose**: Gets the configuration for a specific topic (e.g., retention policy).
- **Query**: `{"operation": "topic_config", "topic": "my-topic"}`
- [IMPORTANT]
====
This operation cannot default to all topics. The generated check logic MUST provide a specific topic name. A common and required pattern is to first use the `list_topics` operation and then iterate through the results, calling `topic_config` for each individual topic.
====

**Pattern 8: `cluster_metadata`**
- **Purpose**: Gets cluster-wide metadata, including broker list and controller ID.
- **Query**: `{"operation": "cluster_metadata"}`

**Pattern 9: `describe_log_dirs`**
- **Purpose**: Gets Kafka's view of on-disk partition sizes for one or more brokers. This is the **only** supported way to check disk usage.
- **Query**: `{"operation": "describe_log_dirs", "broker_ids": []}` (Empty list for all brokers)

**Pattern 10: `list_consumer_group_offsets`**
- **Purpose**: Gets the raw committed offsets for a consumer group without calculating lag.
- **Query**: `{"operation": "list_consumer_group_offsets", "group_id": "my-group"}`


=== Available Data Sources

You can create checks using multiple data sources:

**1. Kafka Admin API (Primary - Always Available)**
Operations: list_topics, describe_topics, consumer_lag, broker_config, etc.
Use for Kafka-specific metrics like topics, partitions, consumer groups.

**2. OS-Level Metrics (Optional - Requires SSH)**
Available when SSH is configured via `ssh_host`, `ssh_user`, `ssh_key_file` settings.

Use `require_ssh()` helper to check availability:
[source,python]
----
from plugins.common.check_helpers import require_ssh
ssh_ok, skip_msg, skip_data = require_ssh(connector, "operation description")
----

Useful for:
- Disk usage: `df -h /var/lib/kafka`
- Process info: `ps aux | grep kafka`
- Network stats: `netstat -s | grep -E "(retransmit|error)"`
- Log file sizes: `du -sh /var/log/kafka/*`

**3. Cloud Provider Metrics (Optional - Requires Cloud Config)**
- AWS CloudWatch (for MSK): Use `require_aws()` helper
- Azure Monitor (for Event Hubs): Use `require_azure()` helper
- Instaclustr API: Use `require_instaclustr()` helper

**CRITICAL: All optional checks MUST use the appropriate `require_*()` helper
and gracefully skip if the service is not configured.**

{% include "shared/os_level_checks_patterns.adoc" %}

=== Required Procedural Pattern for Single-Resource Checks

[IMPORTANT]
====
For operations that act on a single named resource like `topic_config` or `broker_config`, the following two-step procedural pattern is MANDATORY. The AI MUST generate code that follows this exact logic.
====

1.  **The Query Library Function (`qrylib`) MUST accept the resource name as a parameter:**

[source,python]
----
# in plugins/kafka/utils/qrylib/topic_config_queries.py
import json

def get_topic_config_query(connector, topic_name: str):
    """Returns a JSON query to get the config for a SPECIFIC topic."""
    return json.dumps({
        "operation": "topic_config",
        "topic": topic_name  # The topic_name is EMBEDDED in the JSON
    })
----

2.  **The Check Module (`checks`) MUST loop and call the query function for each item:**

[source,python]
----
# in plugins/kafka/checks/topic_configurations.py
from plugins.kafka.utils.qrylib.topic_config_queries import get_topic_config_query
from plugins.kafka.utils.qrylib.list_topics_queries import get_list_topics_query

def run_check_topic_configurations(connector, settings):
    adoc_content = []
    all_configs = {}

    # Step 1: Get the list of all topics
    list_query = get_list_topics_query(connector)
    _, raw_topics = connector.execute_query(list_query, return_raw=True)
    topics = raw_topics.get('topics', [])

    # Step 2: Loop through the list
    for topic_name in topics:
        # Step 3: Call the query function WITH the topic name for EACH iteration
        config_query = get_topic_config_query(connector, topic_name=topic_name)

        # Step 4: Execute the complete query
        _, raw_config = connector.execute_query(config_query, return_raw=True)
        all_configs[topic_name] = raw_config

    # ... process results in all_configs ...
    return "\n".join(adoc_content), {'configs': all_configs}
----


=== Explicit Anti-Patterns to Avoid

[CRITICAL]
====
The AI must strictly avoid the following common failure patterns. Generating code that matches these anti-patterns will result in a failed check.
====

1.  **Anti-Pattern: Forgetting to Pass Required Arguments**

    When a query function in `qrylib` is defined to accept a parameter like `topic_name`, the calling code in the `checks` file **MUST** provide it.

    * ✅ **CORRECT USAGE:**
        [source,python]
        ----
        # The function is called WITH the required 'topic_name' argument
        config_query = get_topic_config_query(connector, topic_name=topic)
        ----

    * ❌ **INCORRECT USAGE (Causes `missing argument` error):**
        [source,python]
        ----
        # The 'topic_name' argument is missing from the function call
        config_query = get_topic_config_query(connector) # WRONG
        ----

2.  **Anti-Pattern: Including Non-Serializable Objects in JSON**

    The `connector` object must NEVER be included in the dictionary that is passed to `json.dumps()`. The JSON payload must only contain simple, serializable data (strings, numbers, lists, dicts).

    * ✅ **CORRECT IMPLEMENTATION:**
        [source,python]
        ----
        def get_topic_config_query(connector, topic_name: str):
            # The connector is used for logic, but NOT included in the output dict
            return json.dumps({
                "operation": "topic_config",
                "topic": topic_name
            })
        ----

    * ❌ **INCORRECT IMPLEMENTATION (Causes `not JSON serializable` error):**
        [source,python]
        ----
        def get_topic_config_query(connector, topic_name: str):
            # The connector object is incorrectly placed inside the dictionary
            return json.dumps({
                "operation": "topic_config",
                "topic": topic_name,
                "connector_object": connector # WRONG
            })
        ----



=== Query Function Patterns

**CRITICAL: Supported Operations**

The KafkaConnector supports these operations via JSON query format:

**Pattern 1: List Topics**

[source,python]
----
import json

def get_topics_query(connector):
    """Returns query for listing all topics."""
    return json.dumps({
        "operation": "list_topics"
    })
----

**Expected Response:**
```python
{
    'topics': ['topic1', 'topic2'],
    'count': 2
}
```

**Note:** Internal topics (starting with `__`) are automatically filtered out by the connector.

**Pattern 2: Describe Topics**

[source,python]
----
import json

def get_topic_details_query(connector):
    """Returns query for topic details."""
    return json.dumps({
        "operation": "describe_topics",
        "topics": []  # Empty = all topics, or specify: ["topic1", "topic2"]
    })
----

**Expected Response:**
```python
[
    {
        'topic': 'my-topic',
        'partitions': 12,
        'replication_factor': 3,
        'under_replicated_partitions': 0
    }
]
```

**Pattern 3: List Consumer Groups**

[source,python]
----
import json

def get_consumer_groups_query(connector):
    """Returns query for consumer groups."""
    return json.dumps({
        "operation": "list_consumer_groups"
    })
----

**Expected Response:**
```python
[
    {'group_id': 'group1', 'protocol_type': 'consumer'},
    {'group_id': 'group2', 'protocol_type': 'consumer'}
]
```

**Note:** This is a list of dictionaries, not a dict with a 'groups' key.

**Pattern 4: Describe Consumer Groups**

[source,python]
----
import json

def get_consumer_group_details_query(connector):
    """Returns query for consumer group details."""
    return json.dumps({
        "operation": "describe_consumer_groups",
        "group_ids": []  # Empty = all groups, or specify: ["group1"]
    })
----

**Expected Response:**
```python
[
    {
        'group_id': 'my-group',
        'state': 'Stable',
        'protocol_type': 'consumer',
        'members': 3
    }
]
```

**Pattern 5: Consumer Lag (Single Group)**

[source,python]
----
import json

def get_consumer_lag_query(connector, group_id: str):
    """Returns query for consumer lag for a specific group."""
    return json.dumps({
        "operation": "consumer_lag",
        "group_id": group_id
    })
----

**Expected Response (Single Group):**
```python
{
    'group_id': 'my-group',
    'details': [
        {
            'group_id': 'my-group',
            'topic': 'topic1',
            'partition': 0,
            'current_offset': 1000,
            'log_end_offset': 1050,
            'lag': 50
        }
    ],
    'total_lag': 50
}
```

**Pattern 6: Consumer Lag (All Groups - Wildcard)**

[source,python]
----
import json

def get_all_consumer_lag_query(connector):
    """Returns query for lag across all consumer groups."""
    return json.dumps({
        "operation": "consumer_lag",
        "group_id": "*"  # Wildcard gets all groups
    })
----

**Expected Response (All Groups with wildcard "*"):**
```python
{
    'group_lags': [
        {
            'group_id': 'group1',
            'topic': 'topic1',
            'partition': 0,
            'current_offset': 1000,
            'log_end_offset': 1050,
            'lag': 50
        },
        {
            'group_id': 'group2',
            'topic': 'topic2',
            'partition': 0,
            'current_offset': 2000,
            'log_end_offset': 2100,
            'lag': 100
        }
    ],
    'total_lag': 150
}
```

**Pattern 7: Broker Configuration**

[source,python]
----
import json

def get_broker_config_query(connector, broker_id: int):
    """Returns query for broker configuration."""
    return json.dumps({
        "operation": "broker_config",
        "broker_id": broker_id
    })
----

**Expected Response:**
```python
{
    'name': '0',
    'configs': {
        'log.retention.hours': '168',
        'num.network.threads': '8',
        'num.io.threads': '8'
    }
}
```

**Pattern 8: Cluster Metadata**

[source,python]
----
import json

def get_cluster_metadata_query(connector):
    """Returns query for cluster metadata."""
    return json.dumps({
        "operation": "cluster_metadata"
    })
----

**Expected Response:**
```python
{
    'cluster_id': 'kafka-cluster-1',
    'controller_id': 0,
    'brokers': [
        {'id': 0, 'host': 'broker1', 'port': 9092},
        {'id': 1, 'host': 'broker2', 'port': 9092}
    ]
}
```

**Note:** The `controller_id` is an integer (not a nested object), representing the broker ID acting as controller.

=== Connector Response Formats Reference

The KafkaConnector returns formatted AsciiDoc strings and raw data structures. All formatting is done inline by the connector (no external AsciiDocFormatter class is used).

**Response Tuple Format:**
When `return_raw=True`, the connector returns: `(formatted_string, raw_dict_or_list)`

**Key Response Structures:**

**1. list_topics:**
```python
{
    'topics': ['topic1', 'topic2'],
    'count': 2
}
```

**2. describe_topics:**
```python
[
    {
        'topic': 'my-topic',
        'partitions': 12,
        'replication_factor': 3,
        'under_replicated_partitions': 0
    }
]
```

**3. list_consumer_groups:**
```python
[
    {'group_id': 'group1', 'protocol_type': 'consumer'}
]
```

**4. describe_consumer_groups:**
```python
[
    {
        'group_id': 'my-group',
        'state': 'Stable',
        'protocol_type': 'consumer',
        'members': 3
    }
]
```

**5. consumer_lag (single group):**
```python
{
    'group_id': 'my-group',
    'details': [
        {
            'group_id': 'my-group',
            'topic': 'my-topic',
            'partition': 0,
            'current_offset': 1000,
            'log_end_offset': 1050,
            'lag': 50
        }
    ],
    'total_lag': 50
}
```

**6. consumer_lag (wildcard "*"):**
```python
{
    'group_lags': [
        {
            'group_id': 'group1',
            'topic': 'topic1',
            'partition': 0,
            'current_offset': 1000,
            'log_end_offset': 1050,
            'lag': 50
        }
    ],
    'total_lag': 50
}
```

**7. broker_config / topic_config:**
```python
{
    'name': '0',  # broker ID or topic name
    'configs': {
        'log.retention.hours': '168',
        'num.network.threads': '8'
    }
}
```

**8. cluster_metadata:**
```python
{
    'cluster_id': 'abc123',
    'controller_id': 0,  # Integer, not nested object
    'brokers': [
        {'id': 0, 'host': 'broker1', 'port': 9092}
    ]
}
```

**9. describe_log_dirs:**
```python
[
    {
        'broker_id': 0,
        'log_dir': '/var/kafka/logs',
        'topic': 'my-topic',
        'partition': 0,
        'size_bytes': 104857600,
        'offset_lag': 0
    }
]
```

**10. list_consumer_group_offsets:**
```python
[
    {
        'group_id': 'my-group',
        'topic': 'my-topic',
        'partition': 0,
        'offset': 1000,
        'metadata': ''
    }
]
```

=== Kafka JSON Trigger Rule Examples

Here are two examples of rule files tailored for common Kafka checks.

==== Example 1: Consumer Lag Analysis

This rule file defines thresholds for alerting on high consumer lag.

*File:* `plugins/kafka/rules/consumer_lag.json`
[source,json]
----
{
  "high_consumer_lag": {
    "metric_keywords": ["kafka", "lag", "consumer", "backlog"],
    "rules": [
      {
        "expression": "int(data.get('lag', 0)) > 10000",
        "level": "critical",
        "score": 9,
        "reasoning": "Consumer group '{data.get('group_id')}' on topic '{data.get('topic')}' has CRITICAL lag of {data.get('lag')} messages, indicating a severe processing delay or stalled consumer.",
        "recommendations": [
          "Immediately investigate the health of the consumers in this group to ensure they are not crashed or stuck.",
          "Consider scaling up the number of consumers for this topic if processing cannot keep up with the message rate."
        ]
      },
      {
        "expression": "int(data.get('lag', 0)) > 1000",
        "level": "high",
        "score": 7,
        "reasoning": "Consumer group '{data.get('group_id')}' on topic '{data.get('topic')}' has high lag of {data.get('lag')} messages, indicating consumers are falling behind producers.",
        "recommendations": [
          "Monitor consumer performance and CPU/memory usage to identify bottlenecks.",
          "Review consumer logs for errors or frequent rebalancing events that could slow down processing."
        ]
      }
    ]
  }
}
----

'''

==== Example 2: Under-Replicated Partitions

This rule file triggers a critical alert if any topic has partitions that are not fully replicated.

*File:* `plugins/kafka/rules/under_replicated_partitions.json`
[source,json]
----
{
  "under_replicated_partitions_found": {
    "metric_keywords": ["kafka", "replication", "availability", "ha", "data-loss-risk"],
    "rules": [
      {
        "expression": "int(data.get('under_replicated_partitions', 0)) > 0",
        "level": "critical",
        "score": 10,
        "reasoning": "Topic '{data.get('topic')}' has {data.get('under_replicated_partitions')} under-replicated partitions. This poses a direct risk of data loss and indicates a broker is down or unreachable.",
        "recommendations": [
          "IMMEDIATE ACTION REQUIRED: Check the status of all Kafka brokers in the cluster.",
          "Investigate network connectivity between brokers to ensure they can communicate for replication.",
          "Review broker logs for errors that may have caused them to fail or shut down."
        ]
      }
    ]
  }
}
----

=== Common Kafka Health Check Patterns

**1. Consumer Lag Check**

Monitors lag per consumer group and alerts on high lag:

[source,python]
----
def run_consumer_lag(connector, settings):
    adoc_content = ["=== Consumer Lag Analysis", ""]
    structured_data = {}
    
    # Get lag threshold from settings
    warning_lag = settings.get('kafka_lag_warning', 1000)
    critical_lag = settings.get('kafka_lag_critical', 10000)
    
    query = get_all_consumer_lag_query(connector)
    formatted, raw = connector.execute_query(query, return_raw=True)
    
    if "[ERROR]" in formatted:
        adoc_content.append(formatted)
        structured_data["consumer_lag"] = {"status": "error"}
    elif raw and raw.get('group_lags'):
        # Analyze lag from group_lags array
        high_lag_groups = [
            item for item in raw['group_lags']
            if item.get('lag', 0) > critical_lag
        ]
        
        if high_lag_groups:
            adoc_content.append("[CRITICAL]\n====\n"
                              f"**Critical Lag Detected:** {len(high_lag_groups)} "
                              f"partitions have lag exceeding {critical_lag}\n"
                              "====\n")
        
        adoc_content.append(formatted)
        structured_data["consumer_lag"] = {
            "status": "success",
            "data": raw['group_lags']
        }
    else:
        adoc_content.append("[NOTE]\n====\n"
                          "No consumer lag detected.\n"
                          "====\n")
        structured_data["consumer_lag"] = {"status": "success", "data": []}
    
    return "\n".join(adoc_content), structured_data
----

**2. Under-Replicated Partitions Check**

Critical check for partition health:

[source,python]
----
def run_under_replicated_partitions(connector, settings):
    adoc_content = ["=== Under-Replicated Partitions", ""]
    structured_data = {}
    
    query = get_topic_details_query(connector)
    formatted, raw = connector.execute_query(query, return_raw=True)
    
    if "[ERROR]" in formatted:
        adoc_content.append(formatted)
        structured_data["under_replicated"] = {"status": "error"}
    elif raw:
        # Filter topics with under-replicated partitions
        under_replicated = [
            t for t in raw
            if t.get('under_replicated_partitions', 0) > 0
        ]
        
        if under_replicated:
            adoc_content.append("[CRITICAL]\n====\n"
                              f"**Data Loss Risk:** {len(under_replicated)} topics "
                              "have under-replicated partitions.\n"
                              "====\n")
            adoc_content.append(formatted)
        else:
            adoc_content.append("[NOTE]\n====\n"
                              "All partitions are fully replicated.\n"
                              "====\n")
        
        structured_data["under_replicated"] = {
            "status": "success",
            "data": raw
        }
    else:
        adoc_content.append("[NOTE]\n====\n"
                          "No topics found.\n"
                          "====\n")
        structured_data["under_replicated"] = {"status": "success", "data": []}
    
    return "\n".join(adoc_content), structured_data
----

**3. Topic Configuration Check**

Validates topic replication and partition settings:

[source,python]
----
def run_topic_configuration(connector, settings):
    adoc_content = ["=== Topic Configuration Analysis", ""]
    structured_data = {}
    
    min_replication = settings.get('kafka_min_replication_factor', 3)
    
    query = get_topic_details_query(connector)
    formatted, raw = connector.execute_query(query, return_raw=True)
    
    if "[ERROR]" in formatted:
        adoc_content.append(formatted)
        structured_data["topic_config"] = {"status": "error"}
    elif raw:
        # Find topics with insufficient replication
        under_replicated = [
            t for t in raw 
            if t.get('replication_factor', 0) < min_replication
        ]
        
        if under_replicated:
            adoc_content.append("[WARNING]\n====\n"
                              f"**Configuration Risk:** {len(under_replicated)} topics "
                              f"have replication factor below {min_replication}\n"
                              "====\n")
        
        adoc_content.append(formatted)
        structured_data["topic_config"] = {"status": "success", "data": raw}
    else:
        adoc_content.append("[NOTE]\n====\n"
                          "No topics found.\n"
                          "====\n")
        structured_data["topic_config"] = {"status": "success", "data": []}
    
    return "\n".join(adoc_content), structured_data
----

=== Critical Monitoring Areas for Kafka

**1. Consumer Lag (HIGH PRIORITY)**
- Indicates consumers falling behind producers
- Can lead to data loss if retention expires
- Threshold: Warn at 1000, Critical at 10000+ messages

**2. Under-Replicated Partitions (CRITICAL)**
- Data loss risk
- Indicates broker or network failures
- Threshold: Any under-replicated partition is critical

**3. ISR (In-Sync Replica) Shrinks (HIGH)**
- Indicates replicas falling out of sync
- Precursor to under-replication
- Monitor rate of ISR shrinkage

**4. Broker Availability**
- Number of active brokers
- Controller elections
- Broker restarts

**5. Topic Configuration**
- Replication factor (minimum 3 recommended)
- Partition count (balanced across brokers)
- Retention settings

**6. Topic & Partition On-Disk Size (using `describe_log_dirs`)**
- Analyzes on-disk size of topic partitions to track storage consumption and identify large topics or data skew.


=== Best Practices

1. **Always filter internal topics** (`__consumer_offsets`, `__transaction_state`)
2. **Use thresholds from settings** for lag and replication warnings
3. **Aggregate metrics by topic** to identify problematic topics
4. **Include time windows** when calculating rates (messages/sec)
5. **Provide actionable recommendations** (scale consumers, increase retention, etc.)
6. **Check broker availability first** as many checks depend on it.
7. **Account for consumer group rebalancing** which can cause temporary lag.

=== Common Pitfalls

1. **Don't query JMX directly** - Use connector's abstraction
2. **Handle empty consumer groups** - Groups may be inactive
3. **Check broker availability first** - Many checks fail if brokers are down
4. **Consider lag context** - 100 messages lag on 1M/sec topic is different than 100 on 1/sec topic
5. **Account for rebalancing** - Consumer groups in "Rebalancing" state temporarily show high lag

== Your Task

Generate a Kafka health check for:

*Plugin Name:* {{ plugin_name }}
*Request:* {{ natural_language_request }}

**Remember:** 
- **Use JSON format** for all queries: `json.dumps({"operation": "...", ...})`
- **For all consumer groups:** Use `"group_id": "*"` in the `consumer_lag` operation.
- **For all topics:** Use an empty list `"topics": []` in the `describe_topics` operation.
- **Import json** at the top of query files

Output ONLY the JSON plan. No explanations, no markdown, no additional text.

=== Metric Keyword Best Practices (CRITICAL)

The `metric_keywords` array determines which metrics a rule applies to. **Overly generic keywords cause rules to be applied to the wrong data**, resulting in evaluation errors and false positives/negatives.

==== The Cross-Contamination Problem

When multiple health checks use overlapping generic keywords, the rule engine will incorrectly apply rules to data from different checks:

**Example Problem:**
```json
// In check_table_bloat.json
{
  "excessive_bloat": {
    "metric_keywords": ["postgres", "table", "bloat"],  // ❌ TOO GENERIC
    "rules": [...]
  }
}

// In check_table_statistics.json  
{
  "outdated_stats": {
    "metric_keywords": ["postgres", "table", "statistics"],  // ❌ TOO GENERIC
    "rules": [...]
  }
}
```

Result: The "excessive_bloat" rule will try to evaluate against table_statistics data (which has no bloat field), causing errors.

==== Keyword Selection Strategy

**ALWAYS include these three types of keywords:**

1. **Check-specific identifier** - Makes the rule unique to this check
   - Use the check function name: `run_table_bloat()` → keyword: `"table_bloat"`
   - Or the primary structured_data key: `structured_data["connection_pool"]` → keyword: `"connection_pool"`

2. **Data structure key** - The actual key in structured_data
   - Example: `structured_data["tables"]` → keyword: `"tables"`
   - Example: `structured_data["indexes"]` → keyword: `"indexes"`

3. **Field-specific terms** - Fields that the rule actually evaluates
   - If rule checks `data.get('bloat_ratio')` → keyword: `"bloat_ratio"`
   - If rule checks `data.get('dead_tuples')` → keyword: `"dead_tuples"`

**AVOID technology-only keywords that appear across ALL checks:**
- ❌ `["postgres"]` - Matches EVERY Postgres check
- ❌ `["cassandra"]` - Matches EVERY Cassandra check
- ❌ `["kafka"]` - Matches EVERY Kafka check
- ❌ `["table"]` - Matches ANY table-related check
- ❌ `["performance"]` - Matches ANY performance check

==== Keyword Construction Formula

**Use this pattern for every rule group:**
```json
"metric_keywords": [
  "<check_name_or_module>",      // Uniquely identifies the check
  "<structured_data_key>",        // The data structure being evaluated
  "<field_name_in_expression>"   // Specific field the rule checks
]
```

==== Technology-Specific Examples

**PostgreSQL Example:**
```json
{
  "excessive_table_bloat": {
    "metric_keywords": ["table_bloat", "bloat_analysis", "bloat_ratio"],
    "rules": [
      {
        "expression": "data.get('bloat_ratio', 0) > 0.3",
        "level": "high",
        "score": 8,
        "reasoning": "Table '{data.get('table_name', 'unknown')}' has {data.get('bloat_ratio', 0):.1%} bloat."
      }
    ]
  }
}
```

**Cassandra Example:**
```json
{
  "compaction_lag": {
    "metric_keywords": ["compaction_status", "pending_tasks", "pending_compactions"],
    "rules": [
      {
        "expression": "data.get('pending_compactions', 0) > 100",
        "level": "high",
        "score": 7,
        "reasoning": "Node has {data.get('pending_compactions', 0)} pending compactions."
      }
    ]
  }
}
```

**MongoDB Example:**
```json
{
  "slow_operations": {
    "metric_keywords": ["current_operations", "operations", "operation_time_ms"],
    "rules": [
      {
        "expression": "data.get('operation_time_ms', 0) > 5000",
        "level": "medium",
        "score": 6,
        "reasoning": "Operation running for {data.get('operation_time_ms', 0)}ms."
      }
    ]
  }
}
```

**Kafka Example:**
```json
{
  "critical_consumer_lag": {
    "metric_keywords": ["consumer_lag", "group_lags", "lag"],
    "rules": [
      {
        "expression": "data.get('lag', 0) > 10000",
        "level": "critical",
        "score": 9,
        "reasoning": "Consumer group '{data.get('group_id', 'unknown')}' has {data.get('lag', 0)} messages of lag."
      }
    ]
  }
}
```

==== Common Patterns by Check Type

**For checks analyzing individual resources (tables, indexes, topics):**
```json
"metric_keywords": [
  "<check_name>",           // e.g., "unused_indexes"
  "<resource_type>",        // e.g., "indexes"
  "<measurement_field>"     // e.g., "index_scans"
]
```

**For checks analyzing aggregated metrics (cluster health, statistics):**
```json
"metric_keywords": [
  "<check_name>",           // e.g., "cluster_health"
  "<metric_category>",      // e.g., "node_status"
  "<specific_metric>"       // e.g., "up_nodes"
]
```

**For checks with multiple data sections:**
```json
// Use different rule groups for different data structures
{
  "rule_for_summary_data": {
    "metric_keywords": ["check_name", "summary", "total_count"],
    "rules": [...]
  },
  "rule_for_detail_data": {
    "metric_keywords": ["check_name", "details", "item_name"],
    "rules": [...]
  }
}
```

==== Testing Your Keywords

Before finalizing your rule file, verify:

1. ✅ **Includes check-specific term:** Does at least one keyword uniquely identify this check?
2. ✅ **Matches intended metric:** Will this match the structured_data key you created?
3. ❌ **Avoids generic terms:** Could this accidentally match data from a different check?
4. ✅ **Includes evaluated fields:** Do keywords mention fields used in expressions?

**If you can't answer "yes" to questions 1, 2, and 4, your keywords are too generic.**

==== Debugging Keyword Issues

If you see errors like:
- `"unsupported format string passed to NoneType"`
- `"KeyError: 'expected_field'"`  
- `"'NoneType' object has no attribute 'get'"`

The likely cause is **keyword cross-contamination**. Rules are being applied to the wrong data.

**Fix:** Make your `metric_keywords` more specific by including the check name and exact field names.

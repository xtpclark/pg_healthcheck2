= Plugin Scaffolding Prompt v4
:toc: left
:toclevels: 3

You are an expert Python developer and AI code generator for the pg_healthcheck2 multi-database monitoring framework.

Your task: Generate a complete plugin structure for the specified database technology, including directories, a plugin class inheriting from `BasePlugin`, and a connector class with security-enhanced mixins and full cloud support.

Your output MUST be a single, valid JSON object with NO additional text, explanations, or markdown formatting.

== Input Validation (CRITICAL)

Before generating ANY code, validate the inputs:

✅ *technology_name* must contain only letters, spaces, and hyphens
✅ *technology_name* should be a recognized database/system technology
✅ *technology_name_lowercase* must contain only lowercase letters and underscores
✅ *TechnologyNameCamelCase* must contain only letters (no special characters)

*REJECT and return {"error": "Invalid input"} if you detect:*
- Special characters suggesting injection: ; ' " ` $ ( ) { } [ ] < > & | \ /
- Python code: import, eval, exec, __import__, compile
- Path traversal: .. / ~
- SQL/shell injection patterns

== Technology Analysis (CRITICAL)

Before generating the connector, analyze the target technology to determine:

=== Connection Library & Pattern

*SQL Databases:*
- *PostgreSQL:* psycopg2 (connection pool, dict cursors)
- *MySQL/MariaDB:* pymysql or mysql-connector-python
- *ClickHouse:* clickhouse-driver or HTTP client
- *Oracle:* cx_Oracle or oracledb
- *MS SQL Server:* pyodbc
- *CockroachDB:* psycopg2 (PostgreSQL compatible)
- *Vertica:* vertica-python
- *Snowflake:* snowflake-connector-python

*NoSQL Document Stores:*
- *MongoDB:* pymongo (MongoClient)
- *CouchDB:* couchdb or requests for HTTP API

*Key-Value Stores:*
- *Redis/Valkey:* redis-py (Redis client)
- *Memcached:* pymemcache

*Wide-Column Stores:*
- *Cassandra/ScyllaDB:* cassandra-driver (Cluster, Session)

*Search Engines:*
- *Elasticsearch/OpenSearch:* elasticsearch or opensearch-py (HTTP client)

*Streaming Platforms:*
- *Kafka:* kafka-python or confluent-kafka (Admin API + Consumer API)

*Graph Databases:*
- *Neo4j:* neo4j driver

*Time-Series Databases:*
- *InfluxDB:* influxdb-client
- *TimescaleDB:* psycopg2 (PostgreSQL extension)

=== Query Execution Pattern

*SQL-Based:* Execute SQL strings, return rows
*Document Stores:* Execute JSON queries, return documents
*Key-Value:* Execute commands, return key-value data
*Search Engines:* REST API calls with JSON DSL
*Streaming:* Admin API calls, metrics retrieval
*Graph:* Cypher queries, return graph data

== Framework Architecture (CRITICAL)

=== Output Structure

The scaffold MUST create ONLY the following:

*Directories:*
- `plugins/{{ technology_name_lowercase }}/`
- `plugins/{{ technology_name_lowercase }}/checks/` (empty)
- `plugins/{{ technology_name_lowercase }}/utils/qrylib/` (empty)
- `plugins/{{ technology_name_lowercase }}/rules/` (empty)
- `plugins/{{ technology_name_lowercase }}/reports/` (empty)
- `plugins/{{ technology_name_lowercase }}/templates/prompts/` (empty)
- `tests/{{ technology_name_lowercase }}/checks/` (empty)

*Files:*
- `plugins/{{ technology_name_lowercase }}/__init__.py` - Complete plugin class
- `plugins/{{ technology_name_lowercase }}/connector.py` - Complete, working connector

*DO NOT CREATE:*
❌ Sample checks (created separately via generate_check_prompt.adoc)
❌ Sample rules (created with checks)
❌ Sample reports (created with checks)
❌ Sample templates (created with checks)
❌ Any test files

The scaffold creates the FOUNDATION ONLY.

== Required File: __init__.py

YOU MUST implement the BasePlugin interface with complete, working code:

[source,python]
----
from pathlib import Path
import json
import logging
from plugins.base import BasePlugin
from .connector import {{ TechnologyNameCamelCase }}Connector

logger = logging.getLogger(__name__)


class {{ TechnologyNameCamelCase }}Plugin(BasePlugin):
    """{{ technology_name }} plugin implementing the BasePlugin interface."""

    @property
    def technology_name(self):
        return "{{ technology_name_lowercase }}"

    def get_connector(self, settings):
        """Returns an instance of the {{ technology_name }} connector."""
        return {{ TechnologyNameCamelCase }}Connector(settings)

    def get_rules_config(self):
        """
        Loads all .json rule files from the rules directory.
        
        Returns:
            dict: All rules merged into a single dictionary
        """
        all_rules = {}
        rules_dir = Path(__file__).parent / 'rules'

        if not rules_dir.is_dir():
            logger.warning(f"Rules directory not found at {rules_dir}")
            return {}

        for rule_file in rules_dir.glob('*.json'):
            try:
                with open(rule_file, 'r') as f:
                    loaded_rules = json.load(f)
                    all_rules.update(loaded_rules)
            except json.JSONDecodeError as e:
                logger.warning(f"Could not parse rule file {rule_file.name}: {e}")
            except IOError as e:
                logger.warning(f"Could not read rule file {rule_file.name}: {e}")

        return all_rules

    def get_report_definition(self, report_config_file=None):
        """
        Loads report definition from a JSON file.
        
        Args:
            report_config_file: Optional path to custom report file
            
        Returns:
            list: List of checks to run
        """
        if report_config_file:
            config_path = Path(report_config_file)
        else:
            config_path = Path(__file__).parent / "reports" / "default_report.json"

        if not config_path.is_file():
            logger.warning(f"Report configuration file not found: {config_path}")
            return []

        try:
            with open(config_path, 'r') as f:
                report_def = json.load(f)
                return report_def.get('checks', [])
        except json.JSONDecodeError as e:
            logger.error(f"Could not parse report file {config_path}: {e}")
            return []
        except Exception as e:
            logger.error(f"Failed to load report definition: {e}")
            return []

    def get_template_path(self) -> Path:
        """Returns the path to this plugin's templates directory."""
        return Path(__file__).parent / "templates"

    def get_module_weights(self) -> dict:
        """
        Returns check category weights for {{ technology_name }}.
        
        Returns:
            dict: Category importance weights (1-10)
        """
        # Customize based on database type:
        # SQL: performance, backup, replication
        # NoSQL: replication, sharding, performance
        # Cache: memory, eviction, persistence
        # Streaming: lag, throughput, availability
        return {
            'performance': 8,
            'security': 9,
            'availability': 10,
            'configuration': 7,
            'capacity': 8
        }

    def get_db_version_from_findings(self, findings: dict) -> str:
        """
        Extracts database version from findings structure.
        
        Args:
            findings: The structured_findings dictionary
            
        Returns:
            str: Database version or "N/A"
        """
        # Try direct key
        if 'version' in findings:
            return findings['version']
        
        # Try metadata
        metadata = findings.get('metadata', {})
        if metadata.get('version'):
            return metadata['version']
        
        # Try version_info check
        version_info = findings.get('version_info', {})
        if version_info.get('version'):
            return version_info['version']
        
        # Try data array pattern
        if version_info.get('data') and isinstance(version_info['data'], list):
            if version_info['data'] and 'version' in version_info['data'][0]:
                return version_info['data'][0]['version']
        
        return "N/A"

    def get_db_name_from_findings(self, findings: dict) -> str:
        """
        Extracts database name from findings structure.
        
        Args:
            findings: The structured_findings dictionary
            
        Returns:
            str: Database name or "N/A"
        """
        # Try direct key
        if 'db_name' in findings:
            return findings['db_name']
        
        # Try metadata
        metadata = findings.get('metadata', {})
        if metadata.get('db_name'):
            return metadata['db_name']
        
        # Try database_info check
        db_info = findings.get('database_info', {})
        if db_info.get('db_name'):
            return db_info['db_name']
        
        return "N/A"
----

== Required File: connector.py

=== Mixin Architecture (Technology-Dependent)

Connector mixin requirements depend on the technology type:

==== Category 1: Pure API Technologies (NO mixins needed)

Technologies that use native client libraries with complete Admin APIs:
- *Kafka:* kafka-python Admin API handles all operations
- *Elasticsearch/OpenSearch:* REST API via HTTP client
- *Redis/Valkey:* redis-py client commands

[source,python]
----
class {{ TechnologyNameCamelCase }}Connector:
    """Simple connector - no mixins needed."""
    
    def __init__(self, settings):
        self.settings = settings
        self.admin_client = None  # or appropriate client
        self.version_info = {}
----

==== Category 2: Databases with Admin Tools (SSH mixin needed)

Technologies requiring SSH for administrative commands:
- *Cassandra/ScyllaDB:* nodetool commands
- *PostgreSQL:* pg_basebackup, pg_controldata
- *MongoDB:* mongostat, mongotop

[source,python]
----
class {{ TechnologyNameCamelCase }}Connector(SSHSupportMixin):
    """Connector with SSH support for admin tools."""
----

==== Category 3: Cloud-Aware Databases (Cloud mixins needed)

Technologies commonly deployed in managed services:
- *PostgreSQL:* RDS, Azure Database, Instaclustr
- *MongoDB:* DocumentDB, Cosmos DB, Atlas
- *MySQL/MariaDB:* RDS, Azure Database

[source,python]
----
class {{ TechnologyNameCamelCase }}Connector(
    SSHSupportMixin,
    AWSSupportMixin,
    AzureSupportMixin,
    InstaclustrSupportMixin
):
    """Full mixin support for flexible deployment."""
----

**Decision Rule:** Use the minimum mixins needed for the technology. Cloud mixins gracefully handle missing configuration (set managers to None), but don't add them unless the technology is commonly deployed in those environments.

=== Required Methods

YOU MUST implement:

1. `__init__(self, settings)` - Initialize with all 4 mixins
2. `connect(self)` - Connect to database and configured services
3. `disconnect(self)` - Close all connections
4. `close(self)` - Alias for disconnect()
5. `execute_query(self, query, params=None, return_raw=False)` - With JSON dispatch
6. `get_db_metadata(self)` - Return {'version': str, 'db_name': str}
7. `_get_version_info(self)` - Parse version information
8. `version_info` property - Expose version dict

=== JSON Query Dispatch Pattern

The execute_query() method MUST support JSON-based operations:

[source,python]
----
def execute_query(self, query, params=None, return_raw=False):
    # Handle JSON operations first
    if isinstance(query, str) and query.strip().startswith('{'):
        try:
            query_obj = json.loads(query)
            operation = query_obj.get('operation')
            
            # Shell commands via SSH
            if operation == 'shell' and self.shell_executor:
                return self.shell_executor.execute(query, return_raw=return_raw)
            
            # AWS CloudWatch metrics
            elif operation == 'cloudwatch' and self.aws_manager:
                dimensions = query_obj.get('dimensions', [])
                metrics = query_obj.get('metrics_to_fetch', [])
                hours = query_obj.get('hours', 24)
                raw_metrics = self.aws_manager.get_cloudwatch_metrics(
                    dimensions, metrics, hours
                )
                formatted = self.formatter.format_dict_as_table(
                    raw_metrics, 'Metric', 'Value'
                )
                return (formatted, raw_metrics) if return_raw else formatted
            
            # Azure Monitor metrics
            elif operation == 'azure_monitor' and self.azure_manager:
                resource_id = query_obj.get('resource_id')
                metrics = query_obj.get('metrics_to_fetch', [])
                hours = query_obj.get('hours', 24)
                raw_metrics = self.azure_manager.get_metrics(
                    resource_id, metrics, hours
                )
                formatted = self.formatter.format_dict_as_table(
                    raw_metrics, 'Metric', 'Value'
                )
                return (formatted, raw_metrics) if return_raw else formatted
            
            # Instaclustr API metrics
            elif operation == 'instaclustr_metrics' and self.instaclustr_manager:
                metric_type = query_obj.get('metric_type', 'health')
                hours = query_obj.get('hours', 24)
                raw_metrics = self.instaclustr_manager.get_metrics(
                    metric_type, hours
                )
                formatted = self.formatter.format_dict_as_table(
                    raw_metrics, 'Metric', 'Value'
                )
                return (formatted, raw_metrics) if return_raw else formatted
                
        except json.JSONDecodeError as e:
            error_msg = self.formatter.format_error(f"Invalid JSON: {e}")
            return (error_msg, {'error': str(e)}) if return_raw else error_msg
        except Exception as e:
            error_msg = self.formatter.format_error(f"Operation failed: {e}")
            return (error_msg, {'error': str(e)}) if return_raw else error_msg
    
    # Then handle native database queries
    # ... database-specific implementation ...
----

=== Security Requirements (CRITICAL)

YOU MUST configure SSH with security defaults:

[source,python]
----
ssh_settings = {
    'ssh_host': settings.get('ssh_host'),
    'ssh_user': settings.get('ssh_user'),
    'ssh_key_file': settings.get('ssh_key_file'),
    'ssh_password': settings.get('ssh_password'),
    'ssh_port': settings.get('ssh_port', 22),
    'ssh_timeout': settings.get('ssh_timeout', 10),
    'ssh_command_timeout': settings.get('ssh_command_timeout', 30),
    'ssh_strict_host_key_checking': settings.get('ssh_strict_host_key_checking', True),  # CRITICAL: True by default
    'ssh_known_hosts_file': settings.get('ssh_known_hosts_file')
}
self.ssh_manager = SSHConnectionManager(ssh_settings)
self.shell_executor = ShellExecutor(
    self.ssh_manager,
    allow_unsafe_commands=settings.get('allow_unsafe_shell_commands', False)  # CRITICAL: False by default
)
----

== Connector Examples by Technology Type

=== Example 1: SQL Database (PostgreSQL Pattern)

This example demonstrates the complete modern architecture for SQL databases:

[source,python]
----
from plugins.common import (
    SSHSupportMixin, SSHConnectionManager, ShellExecutor,
    AWSSupportMixin, AWSConnectionManager,
    AzureSupportMixin, AzureConnectionManager,
    InstaclustrSupportMixin, InstaclustrConnectionManager,
    AsciiDocFormatter
)
import json
import logging
import psycopg2
import psycopg2.extras

logger = logging.getLogger(__name__)


class PostgreSQLConnector(SSHSupportMixin, AWSSupportMixin, AzureSupportMixin, InstaclustrSupportMixin):
    """
    PostgreSQL connector with full mixin support for any deployment scenario.
    """

    def __init__(self, settings):
        self.settings = settings
        self.formatter = AsciiDocFormatter()
        self.client = None
        self._version_info = None
        
        # SSH setup
        if settings.get('ssh_host'):
            ssh_settings = {
                'ssh_host': settings.get('ssh_host'),
                'ssh_user': settings.get('ssh_user'),
                'ssh_key_file': settings.get('ssh_key_file'),
                'ssh_password': settings.get('ssh_password'),
                'ssh_port': settings.get('ssh_port', 22),
                'ssh_timeout': settings.get('ssh_timeout', 10),
                'ssh_command_timeout': settings.get('ssh_command_timeout', 30),
                'ssh_strict_host_key_checking': settings.get('ssh_strict_host_key_checking', True),
                'ssh_known_hosts_file': settings.get('ssh_known_hosts_file')
            }
            self.ssh_manager = SSHConnectionManager(ssh_settings)
            self.shell_executor = ShellExecutor(
                self.ssh_manager,
                allow_unsafe_commands=settings.get('allow_unsafe_shell_commands', False)
            )
        else:
            self.ssh_manager = None
            self.shell_executor = None
        
        # AWS setup
        if settings.get('aws_region'):
            try:
                self.aws_manager = AWSConnectionManager(settings)
            except Exception as e:
                logger.warning(f"AWS setup failed: {e}")
                self.aws_manager = None
        else:
            self.aws_manager = None
        
        # Azure setup
        if settings.get('azure_subscription_id'):
            try:
                self.azure_manager = AzureConnectionManager(settings)
            except Exception as e:
                logger.warning(f"Azure setup failed: {e}")
                self.azure_manager = None
        else:
            self.azure_manager = None
        
        # Instaclustr setup
        if settings.get('instaclustr_api_key'):
            try:
                self.instaclustr_manager = InstaclustrConnectionManager(settings)
            except Exception as e:
                logger.warning(f"Instaclustr setup failed: {e}")
                self.instaclustr_manager = None
        else:
            self.instaclustr_manager = None

    def connect(self):
        """Establishes connections to all configured services."""
        try:
            timeout = self.settings.get('statement_timeout', 30000)
            self.client = psycopg2.connect(
                host=self.settings.get('host', 'localhost'),
                port=self.settings.get('port', 5432),
                dbname=self.settings.get('database', 'postgres'),
                user=self.settings.get('user'),
                password=self.settings.get('password'),
                options=f"-c statement_timeout={timeout}"
            )
            self.client.autocommit = self.settings.get('autocommit', True)
            
            logger.info(f"✅ Connected to {self.settings.get('host')}")
            self._version_info = self._get_version_info()
            
        except psycopg2.Error as e:
            logger.error(f"Database connection failed: {e}")
            raise ConnectionError(f"Could not connect to database: {e}")
        
        if self.has_ssh_support():
            try:
                self.ssh_manager.connect()
            except Exception as e:
                logger.warning(f"SSH connection failed: {e}")

    def disconnect(self):
        """Closes all connections."""
        if self.client:
            try:
                self.client.close()
                logger.info("Database connection closed")
            except Exception as e:
                logger.warning(f"Error during disconnect: {e}")
            finally:
                self.client = None
        
        if self.has_ssh_support():
            self.ssh_manager.disconnect()

    def close(self):
        """Alias for disconnect()."""
        self.disconnect()

    def _get_version_info(self):
        """Fetches and parses database version."""
        try:
            with self.client.cursor() as cursor:
                cursor.execute("SELECT version()")
                version_string = cursor.fetchone()[0]
            
            import re
            version_match = re.search(r'(\d+)\.(\d+)', version_string)
            if version_match:
                major = int(version_match.group(1))
                minor = int(version_match.group(2))
            else:
                major, minor = 0, 0
            
            return {
                'version_string': version_string,
                'major_version': major,
                'minor_version': minor,
                'is_v10_or_newer': major >= 10,
                'is_v12_or_newer': major >= 12,
                'is_v14_or_newer': major >= 14,
                'is_v15_or_newer': major >= 15,
                'is_v17_or_newer': major >= 17,
            }
        except Exception as e:
            logger.warning(f"Could not fetch version: {e}")
            return {'version_string': 'Unknown', 'major_version': 0, 'minor_version': 0}

    @property
    def version_info(self):
        """Returns version information."""
        if self._version_info is None:
            self._version_info = self._get_version_info()
        return self._version_info

    def get_db_metadata(self):
        """
        Fetches basic metadata required by framework.
        
        Returns:
            dict: {'version': str, 'db_name': str}
        """
        try:
            with self.client.cursor() as cursor:
                cursor.execute("SELECT current_database()")
                db_name = cursor.fetchone()[0]
            
            return {
                'version': self.version_info.get('version_string', 'N/A'),
                'db_name': db_name
            }
        except Exception as e:
            logger.warning(f"Could not fetch metadata: {e}")
            return {'version': self.version_info.get('version_string', 'N/A'), 'db_name': 'N/A'}

    def execute_query(self, query, params=None, return_raw=False):
        """
        Executes queries with JSON dispatch support.
        
        Args:
            query: SQL string or JSON operation
            params: Optional query parameters
            return_raw: If True, returns (formatted, raw_data)
        
        Returns:
            str or tuple: Formatted results
        """
        # Handle JSON operations
        if isinstance(query, str) and query.strip().startswith('{'):
            try:
                query_obj = json.loads(query)
                operation = query_obj.get('operation')
                
                if operation == 'shell' and self.shell_executor:
                    return self.shell_executor.execute(query, return_raw=return_raw)
                
                elif operation == 'cloudwatch' and self.aws_manager:
                    dimensions = query_obj.get('dimensions', [])
                    metrics = query_obj.get('metrics_to_fetch', [])
                    hours = query_obj.get('hours', 24)
                    raw_metrics = self.aws_manager.get_cloudwatch_metrics(dimensions, metrics, hours)
                    formatted = self.formatter.format_dict_as_table(raw_metrics, 'Metric', 'Value')
                    return (formatted, raw_metrics) if return_raw else formatted
                
                elif operation == 'azure_monitor' and self.azure_manager:
                    resource_id = query_obj.get('resource_id')
                    metrics = query_obj.get('metrics_to_fetch', [])
                    hours = query_obj.get('hours', 24)
                    raw_metrics = self.azure_manager.get_metrics(resource_id, metrics, hours)
                    formatted = self.formatter.format_dict_as_table(raw_metrics, 'Metric', 'Value')
                    return (formatted, raw_metrics) if return_raw else formatted
                
                elif operation == 'instaclustr_metrics' and self.instaclustr_manager:
                    metric_type = query_obj.get('metric_type', 'health')
                    hours = query_obj.get('hours', 24)
                    raw_metrics = self.instaclustr_manager.get_metrics(metric_type, hours)
                    formatted = self.formatter.format_dict_as_table(raw_metrics, 'Metric', 'Value')
                    return (formatted, raw_metrics) if return_raw else formatted
                
            except json.JSONDecodeError as e:
                error_msg = self.formatter.format_error(f"Invalid JSON: {e}")
                return (error_msg, {'error': str(e)}) if return_raw else error_msg
            except Exception as e:
                error_msg = self.formatter.format_error(f"Operation failed: {e}")
                return (error_msg, {'error': str(e)}) if return_raw else error_msg
        
        # Native SQL execution
        try:
            with self.client.cursor(cursor_factory=psycopg2.extras.DictCursor) as cursor:
                if params:
                    cursor.execute(query, params)
                else:
                    cursor.execute(query)
                
                if cursor.description is None:
                    return ("", []) if return_raw else ""
                
                columns = [desc[0] for desc in cursor.description]
                results = cursor.fetchall()
                raw_results = [dict(row) for row in results]
                
                if not results:
                    formatted = self.formatter.format_note("No results returned.")
                    return (formatted, []) if return_raw else formatted
                
                formatted = self.formatter.format_table(raw_results)
                return (formatted, raw_results) if return_raw else formatted
        
        except psycopg2.Error as e:
            if self.client:
                self.client.rollback()
            logger.error(f"Query failed: {e}", exc_info=True)
            error_msg = self.formatter.format_error(f"Query failed: {e}")
            return (error_msg, {"error": str(e)}) if return_raw else error_msg
----

=== Example 2: Wide-Column Store (Cassandra Pattern)

This example demonstrates distributed database with nodetool support:

[source,python]
----
from plugins.common import (
    SSHSupportMixin, SSHConnectionManager, ShellExecutor,
    AWSSupportMixin, AWSConnectionManager,
    AzureSupportMixin, AzureConnectionManager,
    InstaclustrSupportMixin, InstaclustrConnectionManager,
    AsciiDocFormatter, NodetoolParser
)
import json
import logging
from cassandra.cluster import Cluster
from cassandra.auth import PlainTextAuthProvider
from cassandra.query import dict_factory

logger = logging.getLogger(__name__)


class CassandraConnector(SSHSupportMixin, AWSSupportMixin, AzureSupportMixin, InstaclustrSupportMixin):
    """
    Cassandra connector with full mixin support for any deployment scenario.
    """

    def __init__(self, settings):
        self.settings = settings
        self.formatter = AsciiDocFormatter()
        self.nodetool_parser = NodetoolParser()
        self.cluster = None
        self.session = None
        self._version_info = None
        
        # SSH setup
        if settings.get('ssh_host'):
            ssh_settings = {
                'ssh_host': settings.get('ssh_host'),
                'ssh_user': settings.get('ssh_user'),
                'ssh_key_file': settings.get('ssh_key_file'),
                'ssh_password': settings.get('ssh_password'),
                'ssh_port': settings.get('ssh_port', 22),
                'ssh_timeout': settings.get('ssh_timeout', 10),
                'ssh_command_timeout': settings.get('ssh_command_timeout', 30),
                'ssh_strict_host_key_checking': settings.get('ssh_strict_host_key_checking', True),
                'ssh_known_hosts_file': settings.get('ssh_known_hosts_file')
            }
            self.ssh_manager = SSHConnectionManager(ssh_settings)
            self.shell_executor = ShellExecutor(
                self.ssh_manager,
                allow_unsafe_commands=settings.get('allow_unsafe_shell_commands', False)
            )
        else:
            self.ssh_manager = None
            self.shell_executor = None
        
        # AWS setup (for Keyspaces)
        if settings.get('aws_region'):
            try:
                self.aws_manager = AWSConnectionManager(settings)
            except Exception as e:
                logger.warning(f"AWS setup failed: {e}")
                self.aws_manager = None
        else:
            self.aws_manager = None
        
        # Azure setup
        if settings.get('azure_subscription_id'):
            try:
                self.azure_manager = AzureConnectionManager(settings)
            except Exception as e:
                logger.warning(f"Azure setup failed: {e}")
                self.azure_manager = None
        else:
            self.azure_manager = None
        
        # Instaclustr setup
        if settings.get('instaclustr_api_key'):
            try:
                self.instaclustr_manager = InstaclustrConnectionManager(settings)
            except Exception as e:
                logger.warning(f"Instaclustr setup failed: {e}")
                self.instaclustr_manager = None
        else:
            self.instaclustr_manager = None

    def connect(self):
        """Establishes connections to all configured services."""
        try:
            contact_points = self.settings.get('hosts', ['localhost'])
            port = self.settings.get('port', 9042)
            
            auth_provider = None
            if self.settings.get('user') and self.settings.get('password'):
                auth_provider = PlainTextAuthProvider(
                    username=self.settings.get('user'),
                    password=self.settings.get('password')
                )
            
            self.cluster = Cluster(
                contact_points=contact_points,
                port=port,
                auth_provider=auth_provider
            )
            
            self.session = self.cluster.connect()
            self.session.row_factory = dict_factory
            
            keyspace = self.settings.get('keyspace')
            if keyspace:
                self.session.set_keyspace(keyspace)
            
            logger.info(f"✅ Connected to Cassandra cluster")
            self._version_info = self._get_version_info()
            
        except Exception as e:
            logger.error(f"Database connection failed: {e}")
            raise ConnectionError(f"Could not connect to database: {e}")
        
        if self.has_ssh_support():
            try:
                self.ssh_manager.connect()
            except Exception as e:
                logger.warning(f"SSH connection failed: {e}")

    def disconnect(self):
        """Closes all connections."""
        if self.cluster:
            try:
                self.cluster.shutdown()
                logger.info("Database connection closed")
            except Exception as e:
                logger.warning(f"Error during disconnect: {e}")
            finally:
                self.cluster = None
                self.session = None
        
        if self.has_ssh_support():
            self.ssh_manager.disconnect()

    def close(self):
        """Alias for disconnect()."""
        self.disconnect()

    def _get_version_info(self):
        """Fetches version information."""
        try:
            rows = self.session.execute("SELECT release_version FROM system.local")
            version_string = rows[0]['release_version'] if rows else 'Unknown'
            
            parts = version_string.split('.')
            major = int(parts[0]) if len(parts) > 0 else 0
            
            return {
                'version_string': version_string,
                'major_version': major,
                'is_v3_or_newer': major >= 3,
                'is_v4_or_newer': major >= 4,
                'is_v5_or_newer': major >= 5,
            }
        except Exception as e:
            logger.warning(f"Could not fetch version: {e}")
            return {'version_string': 'Unknown', 'major_version': 0}

    @property
    def version_info(self):
        """Returns version information."""
        if self._version_info is None:
            self._version_info = self._get_version_info()
        return self._version_info

    def get_db_metadata(self):
        """
        Fetches database metadata.
        
        Returns:
            dict: {'version': str, 'db_name': str}
        """
        try:
            keyspace = self.settings.get('keyspace', 'system')
            return {
                'version': self.version_info.get('version_string', 'N/A'),
                'db_name': keyspace
            }
        except Exception as e:
            logger.warning(f"Could not fetch metadata: {e}")
            return {'version': 'N/A', 'db_name': 'N/A'}

    def execute_query(self, query, params=None, return_raw=False):
        """
        Executes CQL queries with JSON dispatch support.
        
        Args:
            query: CQL string or JSON operation
            params: Optional query parameters
            return_raw: If True, returns (formatted, raw_data)
        
        Returns:
            str or tuple: Formatted results
        """
        # Handle JSON operations
        if isinstance(query, str) and query.strip().startswith('{'):
            try:
                query_obj = json.loads(query)
                operation = query_obj.get('operation')
                
                # Nodetool commands
                if operation == 'nodetool' and self.shell_executor:
                    command = query_obj.get('command', '')
                    nodetool_cmd = f"nodetool {command}"
                    
                    stdout, stderr, exit_code = self.ssh_manager.execute_command(nodetool_cmd)
                    
                    if exit_code != 0:
                        error_msg = self.formatter.format_error(f"Nodetool failed: {stderr}")
                        return (error_msg, {'error': stderr}) if return_raw else error_msg
                    
                    parsed = self.nodetool_parser.parse(command, stdout)
                    
                    if command == 'status':
                        formatted = self.formatter.format_nodetool_status(parsed)
                    elif command == 'tpstats':
                        formatted = self.formatter.format_nodetool_tpstats(parsed)
                    elif command == 'compactionstats':
                        formatted = self.formatter.format_nodetool_compactionstats(parsed)
                    else:
                        formatted = self.formatter.format_table(parsed) if isinstance(parsed, list) else str(parsed)
                    
                    return (formatted, parsed) if return_raw else formatted
                
                elif operation == 'shell' and self.shell_executor:
                    return self.shell_executor.execute(query, return_raw=return_raw)
                
                elif operation == 'cloudwatch' and self.aws_manager:
                    dimensions = query_obj.get('dimensions', [])
                    metrics = query_obj.get('metrics_to_fetch', [])
                    hours = query_obj.get('hours', 24)
                    raw_metrics = self.aws_manager.get_cloudwatch_metrics(dimensions, metrics, hours)
                    formatted = self.formatter.format_dict_as_table(raw_metrics, 'Metric', 'Value')
                    return (formatted, raw_metrics) if return_raw else formatted
                
                elif operation == 'azure_monitor' and self.azure_manager:
                    resource_id = query_obj.get('resource_id')
                    metrics = query_obj.get('metrics_to_fetch', [])
                    hours = query_obj.get('hours', 24)
                    raw_metrics = self.azure_manager.get_metrics(resource_id, metrics, hours)
                    formatted = self.formatter.format_dict_as_table(raw_metrics, 'Metric', 'Value')
                    return (formatted, raw_metrics) if return_raw else formatted
                
                elif operation == 'instaclustr_metrics' and self.instaclustr_manager:
                    metric_type = query_obj.get('metric_type', 'health')
                    hours = query_obj.get('hours', 24)
                    raw_metrics = self.instaclustr_manager.get_metrics(metric_type, hours)
                    formatted = self.formatter.format_dict_as_table(raw_metrics, 'Metric', 'Value')
                    return (formatted, raw_metrics) if return_raw else formatted
                
            except json.JSONDecodeError as e:
                error_msg = self.formatter.format_error(f"Invalid JSON: {e}")
                return (error_msg, {'error': str(e)}) if return_raw else error_msg
            except Exception as e:
                error_msg = self.formatter.format_error(f"Operation failed: {e}")
                return (error_msg, {'error': str(e)}) if return_raw else error_msg
        
        # Native CQL execution
        try:
            if params:
                rows = self.session.execute(query, params)
            else:
                rows = self.session.execute(query)
            
            raw_results = list(rows)
            
            if not raw_results:
                formatted = self.formatter.format_note("No results returned.")
                return (formatted, []) if return_raw else formatted
            
            formatted = self.formatter.format_table(raw_results)
            return (formatted, raw_results) if return_raw else formatted
            
        except Exception as e:
            logger.error(f"CQL query failed: {e}")
            error_msg = self.formatter.format_error(f"Query failed: {str(e)}")
            return (error_msg, {'error': str(e)}) if return_raw else error_msg
----

=== Example 3: Document Store (MongoDB Pattern)

This example demonstrates JSON query handling for document databases:

[source,python]
----
from plugins.common import (
    SSHSupportMixin, SSHConnectionManager, ShellExecutor,
    AWSSupportMixin, AWSConnectionManager,
    AzureSupportMixin, AzureConnectionManager,
    InstaclustrSupportMixin, InstaclustrConnectionManager,
    AsciiDocFormatter
)
import json
import logging
from pymongo import MongoClient
from pymongo.errors import ConnectionFailure
from bson import json_util

logger = logging.getLogger(__name__)


class MongoDBConnector(SSHSupportMixin, AWSSupportMixin, AzureSupportMixin, InstaclustrSupportMixin):
    """
    MongoDB connector with full mixin support for any deployment scenario.
    """

    def __init__(self, settings):
        self.settings = settings
        self.formatter = AsciiDocFormatter()
        self.client = None
        self.db = None
        self._version_info = None
        
        # SSH setup
        if settings.get('ssh_host'):
            ssh_settings = {
                'ssh_host': settings.get('ssh_host'),
                'ssh_user': settings.get('ssh_user'),
                'ssh_key_file': settings.get('ssh_key_file'),
                'ssh_password': settings.get('ssh_password'),
                'ssh_port': settings.get('ssh_port', 22),
                'ssh_timeout': settings.get('ssh_timeout', 10),
                'ssh_command_timeout': settings.get('ssh_command_timeout', 30),
                'ssh_strict_host_key_checking': settings.get('ssh_strict_host_key_checking', True),
                'ssh_known_hosts_file': settings.get('ssh_known_hosts_file')
            }
            self.ssh_manager = SSHConnectionManager(ssh_settings)
            self.shell_executor = ShellExecutor(
                self.ssh_manager,
                allow_unsafe_commands=settings.get('allow_unsafe_shell_commands', False)
            )
        else:
            self.ssh_manager = None
            self.shell_executor = None
        
        # AWS setup (for DocumentDB)
        if settings.get('aws_region'):
            try:
                self.aws_manager = AWSConnectionManager(settings)
            except Exception as e:
                logger.warning(f"AWS setup failed: {e}")
                self.aws_manager = None
        else:
            self.aws_manager = None
        
        # Azure setup (for Cosmos DB)
        if settings.get('azure_subscription_id'):
            try:
                self.azure_manager = AzureConnectionManager(settings)
            except Exception as e:
                logger.warning(f"Azure setup failed: {e}")
                self.azure_manager = None
        else:
            self.azure_manager = None
        
        # Instaclustr setup
        if settings.get('instaclustr_api_key'):
            try:
                self.instaclustr_manager = InstaclustrConnectionManager(settings)
            except Exception as e:
                logger.warning(f"Instaclustr setup failed: {e}")
                self.instaclustr_manager = None
        else:
            self.instaclustr_manager = None

    def connect(self):
        """Establishes connections to all configured services."""
        try:
            connection_string = self.settings.get('connection_string')
            if connection_string:
                self.client = MongoClient(connection_string)
            else:
                self.client = MongoClient(
                    host=self.settings.get('host', 'localhost'),
                    port=self.settings.get('port', 27017),
                    username=self.settings.get('user'),
                    password=self.settings.get('password'),
                    authSource=self.settings.get('auth_source', 'admin'),
                    serverSelectionTimeoutMS=self.settings.get('timeout', 30000)
                )
            
            self.db = self.client[self.settings.get('database', 'admin')]
            self.client.admin.command('ping')
            
            logger.info(f"✅ Connected to MongoDB")
            self._version_info = self._get_version_info()
            
        except ConnectionFailure as e:
            logger.error(f"Database connection failed: {e}")
            raise ConnectionError(f"Could not connect to database: {e}")
        
        if self.has_ssh_support():
            try:
                self.ssh_manager.connect()
            except Exception as e:
                logger.warning(f"SSH connection failed: {e}")

    def disconnect(self):
        """Closes all connections."""
        if self.client:
            try:
                self.client.close()
                logger.info("Database connection closed")
            except Exception as e:
                logger.warning(f"Error during disconnect: {e}")
            finally:
                self.client = None
                self.db = None
        
        if self.has_ssh_support():
            self.ssh_manager.disconnect()

    def close(self):
        """Alias for disconnect()."""
        self.disconnect()
        
    def _get_version_info(self):
        """Fetches version information."""
        try:
            build_info = self.client.admin.command('buildInfo')
            version_string = build_info.get('version', 'Unknown')
            
            parts = version_string.split('.')
            major = int(parts[0]) if len(parts) > 0 else 0
            
            return {
                'version_string': version_string,
                'major_version': major,
                'is_v4_or_newer': major >= 4,
                'is_v5_or_newer': major >= 5,
                'is_v6_or_newer': major >= 6,
                'is_v7_or_newer': major >= 7,
            }
        except Exception as e:
            logger.warning(f"Could not fetch version: {e}")
            return {'version_string': 'Unknown', 'major_version': 0}

    @property
    def version_info(self):
        """Returns version information."""
        if self._version_info is None:
            self._version_info = self._get_version_info()
        return self._version_info

    def get_db_metadata(self):
        """
        Fetches database metadata.
        
        Returns:
            dict: {'version': str, 'db_name': str}
        """
        try:
            db_name = self.settings.get('database', 'admin')
            return {
                'version': self.version_info.get('version_string', 'N/A'),
                'db_name': db_name
            }
        except Exception as e:
            logger.warning(f"Could not fetch metadata: {e}")
            return {'version': 'N/A', 'db_name': 'N/A'}

    def execute_query(self, query, params=None, return_raw=False):
        """
        Executes MongoDB queries with JSON dispatch support.
        
        MongoDB query format:
        {
            "collection": "collection_name",
            "operation": "find" | "aggregate" | "count" | "distinct",
            "filter": {...},
            "pipeline": [...],
            "field": "field",
            "limit": 100
        }
        
        Args:
            query: JSON string/dict or operation
            params: Not used
            return_raw: If True, returns (formatted, raw_data)
        
        Returns:
            str or tuple: Formatted results
        """
        # Handle JSON operations
        if isinstance(query, str) and query.strip().startswith('{'):
            try:
                query_obj = json.loads(query)
                operation = query_obj.get('operation')
                
                if operation == 'shell' and self.shell_executor:
                    return self.shell_executor.execute(query, return_raw=return_raw)
                
                elif operation == 'cloudwatch' and self.aws_manager:
                    dimensions = query_obj.get('dimensions', [])
                    metrics = query_obj.get('metrics_to_fetch', [])
                    hours = query_obj.get('hours', 24)
                    raw_metrics = self.aws_manager.get_cloudwatch_metrics(dimensions, metrics, hours)
                    formatted = self.formatter.format_dict_as_table(raw_metrics, 'Metric', 'Value')
                    return (formatted, raw_metrics) if return_raw else formatted
                
                elif operation == 'azure_monitor' and self.azure_manager:
                    resource_id = query_obj.get('resource_id')
                    metrics = query_obj.get('metrics_to_fetch', [])
                    hours = query_obj.get('hours', 24)
                    raw_metrics = self.azure_manager.get_metrics(resource_id, metrics, hours)
                    formatted = self.formatter.format_dict_as_table(raw_metrics, 'Metric', 'Value')
                    return (formatted, raw_metrics) if return_raw else formatted
                
                elif operation == 'instaclustr_metrics' and self.instaclustr_manager:
                    metric_type = query_obj.get('metric_type', 'health')
                    hours = query_obj.get('hours', 24)
                    raw_metrics = self.instaclustr_manager.get_metrics(metric_type, hours)
                    formatted = self.formatter.format_dict_as_table(raw_metrics, 'Metric', 'Value')
                    return (formatted, raw_metrics) if return_raw else formatted
                
                elif 'collection' in query_obj:
                    return self._execute_mongodb_query(query_obj, return_raw)
                
            except json.JSONDecodeError as e:
                error_msg = self.formatter.format_error(f"Invalid JSON: {e}")
                return (error_msg, {'error': str(e)}) if return_raw else error_msg
            except Exception as e:
                error_msg = self.formatter.format_error(f"Operation failed: {e}")
                return (error_msg, {'error': str(e)}) if return_raw else error_msg
        
        elif isinstance(query, dict):
            return self._execute_mongodb_query(query, return_raw)
        
        error_msg = self.formatter.format_error("Invalid query format")
        return (error_msg, {'error': 'invalid_format'}) if return_raw else error_msg

    def _execute_mongodb_query(self, query_obj, return_raw=False):
        """Executes a MongoDB operation."""
        try:
            if 'collection' not in query_obj:
                raise ValueError("Query must include 'collection' field")
            
            collection_name = query_obj['collection']
            operation = query_obj.get('operation', 'find')
            limit = query_obj.get('limit', 100)
            
            collection = self.db[collection_name]
            
            if operation == 'find':
                filter_query = query_obj.get('filter', {})
                projection = query_obj.get('projection', {})
                cursor = collection.find(filter_query, projection).limit(limit)
                raw_results_bson = list(cursor)
            
            elif operation == 'aggregate':
                pipeline = query_obj.get('pipeline', [])
                if not isinstance(pipeline, list):
                    raise ValueError("'pipeline' must be a list")
                cursor = collection.aggregate(pipeline)
                raw_results_bson = list(cursor)
            
            elif operation == 'count':
                filter_query = query_obj.get('filter', {})
                count = collection.count_documents(filter_query)
                raw_results = {'count': count}
                formatted = f"Count: {count}"
                return (formatted, raw_results) if return_raw else formatted
            
            elif operation == 'distinct':
                field = query_obj.get('field', '_id')
                filter_query = query_obj.get('filter', {})
                distinct_values = collection.distinct(field, filter_query)
                raw_results = {
                    'field': field,
                    'distinct_count': len(distinct_values),
                    'values': distinct_values[:100]
                }
                formatted = self.formatter.format_dict_as_table(raw_results, 'Property', 'Value')
                return (formatted, raw_results) if return_raw else formatted
            
            else:
                raise ValueError(f"Unsupported operation: {operation}")
            
            raw_results = json.loads(json_util.dumps(raw_results_bson))
            
            if not raw_results:
                formatted = self.formatter.format_note("No results returned.")
                return (formatted, []) if return_raw else formatted
            
            formatted = self.formatter.format_table(raw_results)
            
            if len(raw_results) == limit:
                formatted += f"\n{self.formatter.format_note(f'Showing first {limit} results.')}"
            
            return (formatted, raw_results) if return_raw else formatted
            
        except Exception as e:
            logger.error(f"MongoDB operation failed: {e}")
            error_msg = self.formatter.format_error(f"MongoDB error: {str(e)}")
            return (error_msg, {'error': str(e)}) if return_raw else error_msg
----

=== Example 4: Streaming Platform (Kafka Pattern)

This example demonstrates pure API-based connectors with no mixins needed:

[source,python]
----
import json
import logging
from kafka import KafkaConsumer
from kafka.admin import KafkaAdminClient, ConfigResource, ConfigResourceType
from kafka.structs import TopicPartition

logger = logging.getLogger(__name__)


class KafkaConnector:
    """Kafka connector using Admin API - no mixins needed."""

    def __init__(self, settings):
        self.settings = settings
        self.admin_client = None
        self.version_info = {}

    def connect(self):
        """Establishes a connection to the Kafka cluster."""
        try:
            bootstrap_servers = self.settings.get('bootstrap_servers', ['localhost:9092'])

            # Handle both list and string formats
            if isinstance(bootstrap_servers, str):
                bootstrap_servers = [s.strip() for s in bootstrap_servers.split(',')]

            self.admin_client = KafkaAdminClient(
                bootstrap_servers=bootstrap_servers,
                client_id='healthcheck_client',
                request_timeout_ms=30000
            )

            self.version_info = self._get_version_info()
            logger.info(f"✅ Connected to Kafka cluster")
            
        except Exception as e:
            logger.error(f"Database connection failed: {e}")
            raise ConnectionError(f"Could not connect to Kafka: {e}")

    def disconnect(self):
        """Closes all connections."""
        if self.admin_client:
            try:
                self.admin_client.close()
                logger.info("Database connection closed")
            except Exception as e:
                logger.warning(f"Error during disconnect: {e}")
            finally:
                self.admin_client = None

    def close(self):
        """Alias for disconnect()."""
        self.disconnect()

    def _get_version_info(self):
        """Fetches broker version information."""
        try:
            cluster_metadata = self.admin_client._client.cluster
            brokers = cluster_metadata.brokers()
            return {
                'version_string': 'Kafka (Version API not supported by client)',
                'broker_count': len(brokers) if brokers else 0
            }
        except Exception as e:
            logger.warning(f"Could not fetch version: {e}")
            return {'version_string': 'Unknown', 'broker_count': 0}

    @property
    def version_info(self):
        """Returns version information."""
        return self._version_info

    def get_db_metadata(self):
        """
        Fetches cluster-level metadata.
        
        Returns:
            dict: {'version': str, 'db_name': str}
        """
        try:
            cluster_info = self.admin_client.describe_cluster()
            cluster_id = cluster_info.get('cluster_id', 'Unknown')
            return {
                'version': self.version_info.get('version_string', 'N/A'),
                'db_name': f"Cluster ID: {cluster_id}"
            }
        except Exception as e:
            logger.warning(f"Could not fetch metadata: {e}")
            return {'version': 'N/A', 'db_name': 'N/A'}

    def execute_query(self, query, params=None, return_raw=False):
        """
        Executes Kafka Admin API operations via JSON dispatch.
        
        Supported operations:
        - list_topics
        - describe_topics
        - list_consumer_groups
        - describe_consumer_groups
        - consumer_lag
        - broker_config
        - topic_config
        - cluster_metadata
        - describe_log_dirs
        - list_consumer_group_offsets
        
        Args:
            query: JSON string defining the operation
            params: Not used (for API compatibility)
            return_raw: If True, returns (formatted, raw_data)
        
        Returns:
            str or tuple: Formatted results
        """
        try:
            query_obj = json.loads(query)
            operation = query_obj.get('operation')

            if operation == 'list_topics':
                return self._list_topics(return_raw)
            elif operation == 'describe_topics':
                return self._describe_topics(query_obj.get('topics', []), return_raw)
            elif operation == 'list_consumer_groups':
                return self._list_consumer_groups(return_raw)
            elif operation == 'describe_consumer_groups':
                return self._describe_consumer_groups(query_obj.get('group_ids', []), return_raw)
            elif operation == 'consumer_lag':
                group_id = query_obj.get('group_id')
                if not group_id:
                    raise ValueError("'consumer_lag' requires 'group_id'")
                if group_id == '*':
                    return self._get_all_consumer_lag(return_raw)
                return self._get_consumer_lag(group_id, return_raw)
            elif operation == 'broker_config':
                broker_id = query_obj.get('broker_id')
                if broker_id is None:
                    raise ValueError("'broker_config' requires 'broker_id'")
                return self._get_broker_config(broker_id, return_raw)
            elif operation == 'topic_config':
                topic = query_obj.get('topic')
                if not topic:
                    raise ValueError("'topic_config' requires 'topic'")
                return self._get_topic_config(topic, return_raw)
            elif operation == 'cluster_metadata':
                return self._get_cluster_metadata(return_raw)
            elif operation == 'describe_log_dirs':
                broker_ids = query_obj.get('broker_ids', [])
                return self._describe_log_dirs(broker_ids, return_raw)
            elif operation == 'list_consumer_group_offsets':
                group_id = query_obj.get('group_id')
                if not group_id:
                    raise ValueError("'list_consumer_group_offsets' requires 'group_id'")
                return self._list_consumer_group_offsets(group_id, return_raw)
            else:
                raise ValueError(f"Unsupported operation: {operation}")

        except json.JSONDecodeError as e:
            msg = f"[ERROR]\n====\nInvalid JSON query: {e}\n====\n"
            logger.error(msg)
            return (msg, {'error': str(e)}) if return_raw else msg
        except Exception as e:
            msg = f"[ERROR]\n====\nOperation failed: {e}\n====\n"
            logger.error(msg, exc_info=True)
            return (msg, {'error': str(e)}) if return_raw else msg

    def _list_topics(self, return_raw=False):
        """Lists all user-visible topics."""
        topics = self.admin_client.list_topics()
        user_topics = sorted([t for t in topics if not t.startswith('__')])
        raw = {'topics': user_topics, 'count': len(user_topics)}
        
        if not user_topics:
            formatted = "[NOTE]\n====\nNo user topics found.\n====\n"
        else:
            formatted = f"User Topics ({len(user_topics)}):\n"
            formatted += "\n".join(f"  - {t}" for t in user_topics)
        
        return (formatted, raw) if return_raw else formatted

    def _describe_topics(self, topics, return_raw=False):
        """Gets detailed information about topics."""
        cluster = self.admin_client._client.cluster
        cluster.request_update()
        
        target_topics = topics or list(cluster.topics(exclude_internal_topics=True))
        raw_results = []

        for topic_name in sorted(target_topics):
            partitions = cluster.partitions_for_topic(topic_name)
            if not partitions:
                continue

            under_replicated = 0
            tp_example = TopicPartition(topic_name, next(iter(partitions)))
            replication_factor = len(cluster.replicas(tp_example))

            for p_id in partitions:
                tp = TopicPartition(topic_name, p_id)
                if len(cluster.in_sync_replicas(tp)) < len(cluster.replicas(tp)):
                    under_replicated += 1
            
            raw_results.append({
                'topic': topic_name,
                'partitions': len(partitions),
                'replication_factor': replication_factor,
                'under_replicated_partitions': under_replicated
            })

        if not raw_results:
            formatted = "[NOTE]\n====\nNo topics found.\n====\n"
        else:
            formatted = "|===\n|Topic|Partitions|Replication Factor|Under-Replicated\n"
            for t in raw_results:
                formatted += f"|{t['topic']}|{t['partitions']}|{t['replication_factor']}|{t['under_replicated_partitions']}\n"
            formatted += "|===\n"
        
        return (formatted, raw_results) if return_raw else formatted

    def _get_consumer_lag(self, group_id, return_raw=False):
        """Calculates consumer lag for a specific group."""
        try:
            committed_offsets = self.admin_client.list_consumer_group_offsets(group_id)
            if not committed_offsets:
                msg = f"[NOTE]\n====\nNo offsets for group '{group_id}'.\n====\n"
                return (msg, {}) if return_raw else msg

            partitions = list(committed_offsets.keys())
            consumer = KafkaConsumer(bootstrap_servers=self.settings.get('bootstrap_servers'))
            end_offsets = consumer.end_offsets(partitions)
            consumer.close()

            lag_data = []
            for partition, offset_meta in committed_offsets.items():
                committed = offset_meta.offset
                end = end_offsets.get(partition, 0)
                lag = max(0, end - committed)
                lag_data.append({
                    'group_id': group_id,
                    'topic': partition.topic,
                    'partition': partition.partition,
                    'current_offset': committed,
                    'log_end_offset': end,
                    'lag': lag
                })
            
            raw = {
                'group_id': group_id,
                'details': lag_data,
                'total_lag': sum(d['lag'] for d in lag_data)
            }
            
            formatted = f"Consumer Group: {group_id}\nTotal Lag: {raw['total_lag']}\n\n"
            formatted += "|===\n|Topic|Partition|Current|End|Lag\n"
            for item in sorted(lag_data, key=lambda x: (x['topic'], x['partition'])):
                formatted += f"|{item['topic']}|{item['partition']}|{item['current_offset']}|{item['log_end_offset']}|{item['lag']}\n"
            formatted += "|===\n"
            
            return (formatted, raw) if return_raw else formatted
            
        except Exception as e:
            msg = f"[ERROR]\n====\nFailed to calculate lag: {e}\n====\n"
            logger.error(msg, exc_info=True)
            return (msg, {'error': str(e)}) if return_raw else msg

    # Additional operation methods (_get_broker_config, _get_topic_config, etc.)
    # would be implemented here following the same pattern...
----

**Key Features:**
- No mixins - pure Kafka Admin API
- JSON-based operation dispatch
- Proper error handling with AsciiDoc formatting
- Wildcard support (`"group_id": "*"`)
- Empty list support (`"topics": []` for all topics)



== JSON String Escaping (CRITICAL)

When generating file content in JSON:

YOU MUST escape all special characters:
- Newlines: `\n` not actual line breaks
- Quotes: `\"` not `"`
- Backslashes: `\\` not `\`

WRONG:
```json
"content": "def hello():
    print('hi')"
```

CORRECT:
```json
"content": "def hello():\n    print('hi')"
```

The entire JSON must be on ONE logical line with escaped newlines.

== Output Format (CRITICAL)

YOU MUST output a JSON object with this exact structure:

**CRITICAL: Escape all newlines, quotes, and special characters in the "content" fields.**
Use `\n` for newlines, `\"` for quotes, `\\` for backslashes.

[source,json]
----
{
  "operations": [
    {"action": "create_directory", "path": "plugins/{{ technology_name_lowercase }}"},
    {"action": "create_directory", "path": "plugins/{{ technology_name_lowercase }}/checks"},
    {"action": "create_directory", "path": "plugins/{{ technology_name_lowercase }}/utils/qrylib"},
    {"action": "create_directory", "path": "plugins/{{ technology_name_lowercase }}/rules"},
    {"action": "create_directory", "path": "plugins/{{ technology_name_lowercase }}/reports"},
    {"action": "create_directory", "path": "plugins/{{ technology_name_lowercase }}/templates/prompts"},
    {"action": "create_directory", "path": "tests/{{ technology_name_lowercase }}/checks"},
    {"action": "create_file", "path": "plugins/{{ technology_name_lowercase }}/__init__.py", "content": "..."},
    {"action": "create_file", "path": "plugins/{{ technology_name_lowercase }}/connector.py", "content": "..."}
  ],
  "integration_step": {
    "target_file_hint": "plugins/__init__.py",
    "instruction": "Add this import to register the plugin",
    "code_snippet_to_add": "from .{{ technology_name_lowercase }} import {{ TechnologyNameCamelCase }}Plugin"
  },
  "post_message": "✅ Successfully scaffolded {{ technology_name }} plugin.\n\nNext steps:\n1. Register: Add 'from .{{ technology_name_lowercase }} import {{ TechnologyNameCamelCase }}Plugin' to plugins/__init__.py\n2. Create checks: ./aidev.py 'add a {{ technology_name_lowercase }} check for [feature]'\n3. Test: Run health check with your configuration"
}
----

== Validation Checklist

Before outputting JSON, verify:

✅ Correct mixin architecture for technology type:
   - Pure API technologies (Kafka, Redis, Elasticsearch): NO mixins
   - SSH admin tools (Cassandra, PostgreSQL): SSHSupportMixin only
   - Cloud-deployable databases: All 4 mixins
✅ Complete execute_query() with appropriate operations:
   - Pure API: Technology-specific operations only
   - With SSH: Add 'shell' operation
   - With cloud: Add 'cloudwatch', 'azure_monitor', 'instaclustr_metrics'
✅ Technology-appropriate operations (not generic placeholders)
✅ AsciiDocFormatter used for all output formatting
✅ No placeholders or TODOs
✅ Real implementations with proper error handling
✅ Exactly 7 directory operations and 2 file operations
✅ No sample checks, rules, reports, or templates


== Your Task

Generate a complete plugin scaffold for:

*Technology Name:* {{ technology_name }}
*Lowercase Name:* {{ technology_name_lowercase }}
*CamelCase Name:* {{ TechnologyNameCamelCase }}

Analyze the technology, select the appropriate database library and connector pattern from the examples above, and generate the complete JSON output.

Output ONLY the JSON. No explanations, no markdown, no additional text.

{
  "pgbouncer_pool_exhaustion_critical": {
    "metric_keywords": ["pgbouncer_health"],
    "data_conditions": [
      { "key": "pools", "exists": true }
    ],
    "rules": [
      {
        "expression": "any(int(pool.get('cl_waiting', 0)) > 10 for pool in data.get('pools', []))",
        "level": "critical",
        "score": 10,
        "reasoning": "PgBouncer pool exhaustion detected - {max((int(pool.get('cl_waiting', 0)) for pool in data.get('pools', [])), default=0)} clients waiting for connections. This causes application failures and timeouts.",
        "recommendations": [
          "URGENT: Pool is severely exhausted - immediate action required.",
          "Clients are waiting for server connections, causing application delays/failures.",
          "Increase default_pool_size immediately in /etc/pgbouncer/pgbouncer.ini",
          "Current default_pool_size: {data.get('config', {}).get('default_pool_size', 'unknown')}",
          "Recommended: Increase by 50% or add 10-20 connections",
          "Also add reserve_pool_size for burst capacity",
          "After changing config: sudo systemctl reload pgbouncer",
          "Monitor: psql -h <host> -p 6432 -U pgbouncer -d pgbouncer -c 'SHOW POOLS'"
        ]
      }
    ]
  },
  "pgbouncer_pool_exhaustion_high": {
    "metric_keywords": ["pgbouncer_health"],
    "data_conditions": [
      { "key": "pools", "exists": true }
    ],
    "rules": [
      {
        "expression": "any(int(pool.get('cl_waiting', 0)) > 5 for pool in data.get('pools', [])) and not any(int(pool.get('cl_waiting', 0)) > 10 for pool in data.get('pools', []))",
        "level": "high",
        "score": 8,
        "reasoning": "{max((int(pool.get('cl_waiting', 0)) for pool in data.get('pools', [])), default=0)} clients waiting for connections. Pool is experiencing exhaustion.",
        "recommendations": [
          "Pool is exhausted - clients are queuing for connections.",
          "This degrades application performance and may cause timeouts.",
          "Increase default_pool_size in /etc/pgbouncer/pgbouncer.ini",
          "Add reserve_pool_size if not configured (recommended: 5-10 connections)",
          "Investigate if there are long-running queries holding connections",
          "Check application connection handling - ensure connections are properly released",
          "Reload after changes: sudo systemctl reload pgbouncer"
        ]
      }
    ]
  },
  "pgbouncer_pool_exhaustion_warning": {
    "metric_keywords": ["pgbouncer_health"],
    "data_conditions": [
      { "key": "pools", "exists": true }
    ],
    "rules": [
      {
        "expression": "any(int(pool.get('cl_waiting', 0)) > 0 for pool in data.get('pools', [])) and not any(int(pool.get('cl_waiting', 0)) > 5 for pool in data.get('pools', []))",
        "level": "warning",
        "score": 5,
        "reasoning": "{sum(int(pool.get('cl_waiting', 0)) for pool in data.get('pools', []))} client(s) waiting for connections. Pool is beginning to experience pressure.",
        "recommendations": [
          "Some clients are waiting for connections - early sign of pool pressure.",
          "Monitor pool utilization closely.",
          "Consider increasing default_pool_size proactively.",
          "Add reserve_pool_size if not configured.",
          "Review application connection patterns.",
          "Check SHOW POOLS regularly to monitor wait queue."
        ]
      }
    ]
  },
  "pgbouncer_high_wait_time": {
    "metric_keywords": ["pgbouncer_health"],
    "data_conditions": [
      { "key": "analysis", "exists": true }
    ],
    "rules": [
      {
        "expression": "data.get('analysis', {}).get('max_wait_time', 0) > 5.0",
        "level": "critical",
        "score": 9,
        "reasoning": "Maximum client wait time is {data.get('analysis', {}).get('max_wait_time', 0):.2f} seconds. This causes severe application performance degradation.",
        "recommendations": [
          "URGENT: Clients are waiting {data.get('analysis', {}).get('max_wait_time', 0):.2f}s for connections.",
          "This indicates either: (1) Pool too small, or (2) Slow queries holding connections",
          "Immediate actions:",
          "  1. Increase pool size: default_pool_size += 10-20",
          "  2. Check for long-running queries: SELECT * FROM pg_stat_activity WHERE state = 'active' AND query_start < now() - interval '30 seconds'",
          "  3. Review slow query log",
          "  4. Add reserve_pool if not present",
          "High wait times directly translate to application timeouts and errors."
        ]
      },
      {
        "expression": "data.get('analysis', {}).get('max_wait_time', 0) > 1.0 and data.get('analysis', {}).get('max_wait_time', 0) <= 5.0",
        "level": "high",
        "score": 6,
        "reasoning": "Maximum client wait time is {data.get('analysis', {}).get('max_wait_time', 0):.2f} seconds. This degrades application performance.",
        "recommendations": [
          "Clients waiting over 1 second for connections - performance impact likely.",
          "Increase pool size or investigate slow query execution.",
          "Monitor query performance via pg_stat_statements.",
          "Consider adding more server connections to the pool."
        ]
      }
    ]
  },
  "pgbouncer_high_pool_utilization": {
    "metric_keywords": ["pgbouncer_health"],
    "data_conditions": [
      { "key": "pools", "exists": true }
    ],
    "rules": [
      {
        "expression": "any((int(pool.get('sv_active', 0)) + int(pool.get('sv_used', 0))) / max(int(pool.get('sv_active', 0)) + int(pool.get('sv_idle', 0)) + int(pool.get('sv_used', 0)), 1) > 0.9 for pool in data.get('pools', []))",
        "level": "warning",
        "score": 5,
        "reasoning": "One or more pools are at >90% utilization. Pool is near capacity and at risk of exhaustion.",
        "recommendations": [
          "Pool utilization is very high - approaching exhaustion.",
          "Proactively increase pool size before clients start waiting.",
          "Add reserve_pool_size for burst capacity: reserve_pool_size = {max(5, int(data.get('config', {}).get('default_pool_size', 20)) // 4)}",
          "Monitor pool status more frequently during peak hours.",
          "High sustained utilization may indicate undersized pool."
        ]
      }
    ]
  },
  "pgbouncer_no_reserve_pool": {
    "metric_keywords": ["pgbouncer_health"],
    "data_conditions": [
      { "key": "config", "exists": true }
    ],
    "rules": [
      {
        "expression": "int(data.get('config', {}).get('reserve_pool_size', 0)) == 0 and int(data.get('config', {}).get('max_client_conn', 100)) > int(data.get('config', {}).get('default_pool_size', 20)) * 5",
        "level": "warning",
        "score": 4,
        "reasoning": "No reserve pool configured. Burst traffic can cause pool exhaustion without a reserve pool buffer.",
        "recommendations": [
          "Configure reserve_pool_size to handle traffic bursts.",
          "Recommended: reserve_pool_size = {max(5, int(data.get('config', {}).get('default_pool_size', 20)) // 2)}",
          "Reserve pool provides extra connections when main pool is exhausted.",
          "Edit /etc/pgbouncer/pgbouncer.ini:",
          "  [pgbouncer]",
          "  reserve_pool_size = {max(5, int(data.get('config', {}).get('default_pool_size', 20)) // 2)}",
          "  reserve_pool_timeout = 5",
          "Reload: sudo systemctl reload pgbouncer"
        ]
      }
    ]
  },
  "pgbouncer_health_score_low": {
    "metric_keywords": ["pgbouncer_health"],
    "data_conditions": [
      { "key": "health_score", "exists": true }
    ],
    "rules": [
      {
        "expression": "data.get('health_score', 100) < 50",
        "level": "critical",
        "score": 9,
        "reasoning": "PgBouncer health score is {data.get('health_score', 100)}/100. Critical issues detected that require immediate attention.",
        "recommendations": [
          "URGENT: PgBouncer health is critically low.",
          "Review all detected issues in the PgBouncer Health Status section.",
          "Address pool exhaustion and high wait times immediately.",
          "PgBouncer issues directly cause application failures and performance problems.",
          "Common fixes:",
          "  • Increase default_pool_size",
          "  • Add reserve_pool_size",
          "  • Investigate and optimize slow queries",
          "  • Review application connection handling"
        ]
      },
      {
        "expression": "data.get('health_score', 100) < 70 and data.get('health_score', 100) >= 50",
        "level": "high",
        "score": 6,
        "reasoning": "PgBouncer health score is {data.get('health_score', 100)}/100. Significant issues detected that should be addressed soon.",
        "recommendations": [
          "PgBouncer health is degraded - address issues during next maintenance window.",
          "Review detected issues in the PgBouncer Health Status section.",
          "Monitor pool utilization and wait queues closely.",
          "Plan pool size increases if utilization is consistently high."
        ]
      },
      {
        "expression": "data.get('health_score', 100) < 90 and data.get('health_score', 100) >= 70",
        "level": "warning",
        "score": 3,
        "reasoning": "PgBouncer health score is {data.get('health_score', 100)}/100. Minor issues detected - consider optimization.",
        "recommendations": [
          "PgBouncer has minor issues that could be optimized.",
          "Review recommendations in the PgBouncer Health Status section.",
          "Consider configuration tuning during next maintenance."
        ]
      }
    ]
  },
  "pgbouncer_session_mode_inefficient": {
    "metric_keywords": ["pgbouncer_health"],
    "data_conditions": [
      { "key": "config", "exists": true }
    ],
    "rules": [
      {
        "expression": "data.get('config', {}).get('pool_mode') == 'session'",
        "level": "info",
        "score": 1,
        "reasoning": "PgBouncer is using 'session' pool mode - the least efficient but most compatible mode.",
        "recommendations": [
          "Session mode holds connections for entire client session - least efficient pooling.",
          "If your application is stateless (no prepared statements, temp tables, session variables):",
          "  • Consider switching to 'transaction' mode for much better connection reuse",
          "  • Transaction mode releases connection after each transaction",
          "  • Can reduce required pool size significantly",
          "Incompatibilities with transaction mode:",
          "  • Named prepared statements",
          "  • TEMP tables",
          "  • SET/RESET commands",
          "  • Advisory locks",
          "  • LISTEN/NOTIFY",
          "Test in non-production first before changing pool mode."
        ]
      }
    ]
  },
  "pgbouncer_connection_imbalance": {
    "metric_keywords": ["pgbouncer_health"],
    "data_conditions": [
      { "key": "pools", "exists": true }
    ],
    "rules": [
      {
        "expression": "any((int(pool.get('cl_active', 0)) + int(pool.get('cl_waiting', 0))) > (int(pool.get('sv_active', 0)) + int(pool.get('sv_idle', 0)) + int(pool.get('sv_used', 0))) * 20 for pool in data.get('pools', []) if (int(pool.get('sv_active', 0)) + int(pool.get('sv_idle', 0)) + int(pool.get('sv_used', 0))) > 0)",
        "level": "info",
        "score": 1,
        "reasoning": "High client-to-server connection ratio detected. This is normal for transaction pooling but monitor for wait times.",
        "recommendations": [
          "Client-to-server ratio is very high (>20:1).",
          "This is normal and expected for transaction mode pooling.",
          "The benefit of pooling is that many clients share few server connections.",
          "Monitor for client wait queues (cl_waiting) to ensure pool isn't undersized.",
          "If wait times are observed, increase pool size rather than being concerned about ratio."
        ]
      }
    ]
  },
  "pgbouncer_reconnection_severe": {
    "metric_keywords": ["pgbouncer_health"],
    "data_conditions": [
      { "key": "analysis", "exists": true }
    ],
    "rules": [
      {
        "expression": "data.get('analysis', {}).get('reconnection_events', {}).get('count', 0) >= 20",
        "level": "critical",
        "score": 10,
        "reasoning": "SEVERE PgBouncer connection instability: PgBouncer connection dropped {data.get('analysis', {}).get('reconnection_events', {}).get('count', 0)} times during health check execution. This indicates catastrophic infrastructure failure.",
        "recommendations": [
          "URGENT: PgBouncer connection is critically unstable - IMMEDIATE action required.",
          "Connection dropping this frequently WILL cause application failures.",
          "Immediate Investigation Steps:",
          "  1. Check PgBouncer logs: ssh <pgbouncer-host> 'sudo tail -100 /var/log/pgbouncer/pgbouncer.log | grep -iE \"close|disconnect|error|fatal\"'",
          "  2. Check PostgreSQL backend logs: sudo tail -100 /var/log/postgresql/postgresql-*.log | grep -iE 'fatal|error|closing|disconnect'",
          "  3. Verify SSL/TLS configuration between PgBouncer and PostgreSQL",
          "  4. Check for max_connections exhaustion on backend PostgreSQL",
          "  5. Investigate network stability between PgBouncer and PostgreSQL servers",
          "Common Causes:",
          "  • SSL/TLS handshake failures between PgBouncer and PostgreSQL",
          "  • Backend PostgreSQL closing connections (statement_timeout, idle_in_transaction_session_timeout)",
          "  • max_connections exhaustion on PostgreSQL",
          "  • Network instability or firewall dropping connections",
          "  • Aggressive PgBouncer timeout settings (server_idle_timeout too low)"
        ]
      }
    ]
  },
  "pgbouncer_reconnection_critical": {
    "metric_keywords": ["pgbouncer_health"],
    "data_conditions": [
      { "key": "analysis", "exists": true }
    ],
    "rules": [
      {
        "expression": "data.get('analysis', {}).get('reconnection_events', {}).get('count', 0) >= 10 and data.get('analysis', {}).get('reconnection_events', {}).get('count', 0) < 20",
        "level": "critical",
        "score": 8,
        "reasoning": "Critical PgBouncer connection instability: PgBouncer connection dropped {data.get('analysis', {}).get('reconnection_events', {}).get('count', 0)} times during health check execution. This indicates significant infrastructure problems.",
        "recommendations": [
          "URGENT: PgBouncer connection is very unstable - requires immediate investigation.",
          "This level of instability will degrade application performance and may cause failures.",
          "Investigation Steps:",
          "  1. Review PgBouncer logs for connection errors",
          "  2. Check PostgreSQL backend server health and logs",
          "  3. Verify SSL/TLS configuration is correct",
          "  4. Check network stability between PgBouncer and PostgreSQL",
          "  5. Review timeout settings: server_idle_timeout, query_timeout",
          "  6. Verify backend max_connections is not exhausted",
          "Immediate Actions:",
          "  • If SSL errors: Fix certificate configuration or disable SSL temporarily to isolate",
          "  • If timeout errors: Increase server_idle_timeout in pgbouncer.ini",
          "  • If max_connections: Increase PostgreSQL max_connections",
          "  • If network: Check firewall rules and TCP keep-alive settings"
        ]
      }
    ]
  },
  "pgbouncer_reconnection_high": {
    "metric_keywords": ["pgbouncer_health"],
    "data_conditions": [
      { "key": "analysis", "exists": true }
    ],
    "rules": [
      {
        "expression": "data.get('analysis', {}).get('reconnection_events', {}).get('count', 0) >= 5 and data.get('analysis', {}).get('reconnection_events', {}).get('count', 0) < 10",
        "level": "high",
        "score": 6,
        "reasoning": "Significant PgBouncer connection instability: PgBouncer connection dropped {data.get('analysis', {}).get('reconnection_events', {}).get('count', 0)} times during health check execution.",
        "recommendations": [
          "PgBouncer connection is experiencing notable instability.",
          "This should be investigated and resolved soon to prevent degradation.",
          "Investigation Steps:",
          "  1. Check PgBouncer logs: SHOW STATS; in pgbouncer console",
          "  2. Review PostgreSQL logs for connection-related errors",
          "  3. Check SSL/TLS configuration",
          "  4. Monitor network between PgBouncer and PostgreSQL",
          "Common Fixes:",
          "  • Increase server_idle_timeout if connections timing out",
          "  • Fix SSL certificate issues if SSL errors present",
          "  • Optimize slow queries if statement_timeout triggering",
          "  • Check for intermittent network issues"
        ]
      }
    ]
  },
  "pgbouncer_reconnection_warning": {
    "metric_keywords": ["pgbouncer_health"],
    "data_conditions": [
      { "key": "analysis", "exists": true }
    ],
    "rules": [
      {
        "expression": "data.get('analysis', {}).get('reconnection_events', {}).get('count', 0) >= 1 and data.get('analysis', {}).get('reconnection_events', {}).get('count', 0) < 5",
        "level": "warning",
        "score": 3,
        "reasoning": "Minor PgBouncer connection instability: PgBouncer connection dropped {data.get('analysis', {}).get('reconnection_events', {}).get('count', 0)} time(s) during health check execution.",
        "recommendations": [
          "PgBouncer experienced minor connection instability.",
          "Monitor for recurring issues - may indicate intermittent problems.",
          "Check Points:",
          "  • Review PgBouncer logs for patterns",
          "  • Check for transient network issues",
          "  • Verify SSL certificate validity",
          "  • Monitor if this becomes more frequent",
          "If reconnections continue or increase:",
          "  • Investigate SSL/TLS configuration",
          "  • Review timeout settings",
          "  • Check backend PostgreSQL health"
        ]
      }
    ]
  },
  "pgbouncer_query_fallback_critical": {
    "metric_keywords": ["pgbouncer_health"],
    "data_conditions": [
      { "key": "analysis", "exists": true }
    ],
    "rules": [
      {
        "expression": "data.get('analysis', {}).get('fallback_events', {}).get('count', 0) > 0",
        "level": "critical",
        "score": 10,
        "reasoning": "CRITICAL: {data.get('analysis', {}).get('fallback_events', {}).get('count', 0)} queries failed through PgBouncer but succeeded via direct connection. PgBouncer is NOT functioning correctly.",
        "recommendations": [
          "URGENT: PgBouncer is failing to execute queries correctly!",
          "Queries are only succeeding because direct fallback is configured.",
          "WITHOUT direct fallback, your application would be experiencing FAILURES.",
          "This indicates PgBouncer is fundamentally broken - immediate action required.",
          "Investigation Steps:",
          "  1. Check which queries are failing in the PgBouncer Health Status section",
          "  2. Review PgBouncer logs for query execution errors",
          "  3. Verify PgBouncer pool configuration and status",
          "  4. Check for query compatibility issues with PgBouncer pool mode",
          "Common Causes:",
          "  • PgBouncer pool exhaustion preventing query execution",
          "  • Query incompatible with current pool mode (transaction vs session)",
          "  • PgBouncer configuration errors",
          "  • Backend connection failures",
          "IMMEDIATE ACTION: Review failed queries and fix PgBouncer configuration."
        ]
      }
    ]
  }
}

= Common Utilities Documentation
:toc: left
:toclevels: 3
:icons: font

== Overview

The `plugins/common` module provides shared utilities and helper functions used across all database plugins in the pg_healthcheck2 framework. These utilities promote code reuse, consistency, and maintainability by implementing the DRY (Don't Repeat Yourself) principle.

**Key Features:**

* ‚úÖ SSH connection management with security features
* ‚úÖ Shell command execution with sanitization
* ‚úÖ Robust output parsing with regex-based validation
* ‚úÖ AsciiDoc formatting with proper escaping
* ‚úÖ Cloud platform integrations (AWS, Azure, Instaclustr)
* ‚úÖ Retry logic for transient failures
* ‚úÖ Type-safe helper functions

== Module Architecture

[source,text]
----
plugins/common/
‚îú‚îÄ‚îÄ __init__.py               # Package initialization and exports
‚îú‚îÄ‚îÄ ssh_handler.py            # SSH connection management with security
‚îú‚îÄ‚îÄ ssh_mixin.py              # SSH support detection for connectors
‚îú‚îÄ‚îÄ shell_executor.py         # Command execution with sanitization
‚îú‚îÄ‚îÄ parsers.py                # Output parsing (nodetool, shell) with regex
‚îú‚îÄ‚îÄ output_formatters.py      # AsciiDoc formatting with escaping
‚îú‚îÄ‚îÄ check_helpers.py          # Reusable check utilities with type hints
‚îú‚îÄ‚îÄ retry_utils.py            # Retry logic for API calls
‚îú‚îÄ‚îÄ aws_handler.py            # AWS RDS and CloudWatch integration
‚îú‚îÄ‚îÄ azure_handler.py          # Azure PostgreSQL and Monitor integration
‚îî‚îÄ‚îÄ instaclustr_handler.py    # Instaclustr API integration
----

== SSH Infrastructure

=== ssh_handler.py

Manages SSH connections to remote database servers using Paramiko with enhanced security.

==== SSHConnectionManager

Thread-safe SSH connection manager with automatic reconnection and host key verification.

*Features:*
- ‚úÖ Persistent SSH connections with keep-alive
- ‚úÖ Automatic reconnection on connection loss
- ‚úÖ Thread-safe connection management
- ‚úÖ Support for key-based and password authentication
- ‚úÖ **Secure host key verification (production mode)**
- ‚úÖ Configurable timeouts (connection and command)
- ‚úÖ Graceful connection cleanup

*Configuration:*
[source,python]
----
# Production configuration (secure)
settings = {
    'ssh_host': 'database-server.example.com',
    'ssh_user': 'ubuntu',
    'ssh_key_file': '/home/user/.ssh/id_rsa',
    'ssh_port': 22,                              # Optional, default: 22
    'ssh_timeout': 10,                           # Connection timeout in seconds
    'ssh_command_timeout': 30,                   # Command timeout in seconds
    'ssh_strict_host_key_checking': True,        # Enable host key verification (default)
    'ssh_known_hosts_file': '/path/to/known_hosts'  # Optional custom known_hosts
}

# Development/testing configuration (INSECURE!)
settings = {
    'ssh_host': '192.168.1.113',
    'ssh_user': 'cassandra',
    'ssh_key_file': '/home/user/.ssh/id_rsa',
    'ssh_strict_host_key_checking': False       # ‚ö†Ô∏è Only for dev/test!
}

ssh_manager = SSHConnectionManager(settings)
----

*Basic Usage:*
[source,python]
----
from plugins.common import SSHConnectionManager

# Initialize
ssh_manager = SSHConnectionManager(settings)

# Connect
ssh_manager.connect()

# Execute command
stdout, stderr, exit_code = ssh_manager.execute_command('df -h')

# Cleanup
ssh_manager.disconnect()
----

*With Context Manager:*
[source,python]
----
with SSHConnectionManager(settings) as ssh:
    stdout, stderr, exit_code = ssh.execute_command('nodetool status')
    if exit_code == 0:
        print(stdout)
----

*Security Features:*

[cols="1,3"]
|===
|Feature |Description

|Host Key Verification
|Uses `RejectPolicy()` by default to prevent MITM attacks

|Known Hosts Loading
|Loads system and custom known_hosts files

|Secure by Default
|`ssh_strict_host_key_checking=True` unless explicitly disabled

|Command Timeouts
|Separate timeouts for connection and command execution

|Audit Logging
|Logs all connection attempts and command executions
|===

*Error Handling:*
[source,python]
----
try:
    ssh_manager.connect()
except ConnectionError as e:
    logger.error(f"SSH connection failed: {e}")
    # Handle connection failure
except TimeoutError as e:
    logger.error(f"SSH connection timeout: {e}")
    # Handle timeout
----

*Key Methods:*

[cols="1,3"]
|===
|Method |Description

|`connect()`
|Establishes SSH connection with host key verification

|`disconnect()`
|Closes SSH connection gracefully

|`ensure_connected()`
|Verifies connection is active, reconnects if needed

|`execute_command(command, timeout=30)`
|Executes command and returns (stdout, stderr, exit_code)

|`is_connected()`
|Returns True if connection is active
|===

=== ssh_mixin.py

Provides SSH capability detection for database connectors.

==== SSHSupportMixin

Mixin class that adds SSH support detection to connectors.

*Usage in Connectors:*
[source,python]
----
from plugins.common import SSHConnectionManager, SSHSupportMixin

class MyDatabaseConnector(SSHSupportMixin):
    def __init__(self, settings):
        self.settings = settings
        
        # Initialize SSH if configured
        if settings.get('ssh_host'):
            self.ssh_manager = SSHConnectionManager(settings)
    
    def connect(self):
        # Connect to database
        # ...
        
        # Connect SSH if available
        if self.has_ssh_support():
            self.ssh_manager.connect()
----

*Checking SSH Availability:*
[source,python]
----
connector = MyDatabaseConnector(settings)

if connector.has_ssh_support():
    # Execute SSH-based operations
    stdout, stderr, exit_code = connector.ssh_manager.execute_command('df -h')
else:
    # Skip SSH-based checks
    print("SSH not configured, skipping system-level checks")
----

*Key Methods:*

[cols="1,3"]
|===
|Method |Description

|`has_ssh_support()`
|Returns True if connector has SSH configured and connected

|`get_ssh_skip_message(operation_name)`
|Returns formatted skip message for checks requiring SSH

|`get_ssh_settings_info()`
|Returns SSH configuration info (safe for logging)
|===

== Command Execution

=== shell_executor.py

Utilities for executing and processing shell commands with security sanitization.

==== ShellExecutor

Executor class for shell commands with built-in security and safety features.

*Features:*
- ‚úÖ Command whitelisting for security
- ‚úÖ Dangerous pattern detection (`;`, `&&`, `||`, backticks, etc.)
- ‚úÖ Smart empty result handling
- ‚úÖ Extensible operation handlers
- ‚úÖ Optional unsafe mode for trusted environments

*Execute Command:*
[source,python]
----
from plugins.common import ShellExecutor, SSHConnectionManager

ssh_manager = SSHConnectionManager(settings)
ssh_manager.connect()

# Secure mode (default)
executor = ShellExecutor(ssh_manager)

# Execute safe command
result = executor.execute('{"operation": "shell", "command": "df -h"}')

# Unsafe mode (for trusted environments only!)
executor_unsafe = ShellExecutor(ssh_manager, allow_unsafe_commands=True)
----

*Safe Commands Whitelist:*
[source,python]
----
SAFE_COMMANDS = {
    'df', 'free', 'ps', 'uptime', 'w', 'top', 'vmstat', 'iostat',
    'netstat', 'ss', 'lsof', 'dmesg', 'journalctl', 'systemctl',
    'nodetool', 'cqlsh', 'redis-cli', 'mongo', 'mongosh',
    'du', 'ls', 'find', 'grep', 'awk', 'sed', 'cat', 'tail', 'head',
    'wc', 'sort', 'uniq', 'hostname', 'uname', 'whoami', 'id'
}
----

*Empty Result Detection:*
[source,python]
----
# Commands that may legitimately return empty results
EMPTY_OK_COMMANDS = {
    'find', 'grep', 'locate', 'ls', 'awk', 'sed', 'lsof'
}
----

*Register Custom Operations:*
[source,python]
----
def custom_mysql_handler(command: str) -> Tuple[str, Any]:
    # Custom handler logic
    return formatted_output, raw_data

executor.register_operation('mysql-admin', custom_mysql_handler)

# Now you can use it
result = executor.execute('{"operation": "mysql-admin", "command": "status"}')
----

*Security Validation:*
[source,python]
----
# These commands are BLOCKED by default:
executor.execute('{"operation": "shell", "command": "rm -rf /; echo hacked"}')  
# Raises ValueError: dangerous pattern ';' detected

executor.execute('{"operation": "shell", "command": "cat /etc/passwd && malicious"}')
# Raises ValueError: dangerous pattern '&&' detected

# Safe commands are allowed:
executor.execute('{"operation": "shell", "command": "df -h"}')  # ‚úÖ Allowed
executor.execute('{"operation": "shell", "command": "find /tmp -name *.log"}')  # ‚úÖ Allowed
----

*Key Methods:*

[cols="1,2,2"]
|===
|Method |Parameters |Returns

|`execute(query, return_raw)`
|JSON query string, raw flag
|Formatted output or (formatted, raw) tuple

|`register_operation(name, handler)`
|Operation name, handler function
|None (registers handler)

|`_sanitize_command(command)`
|Command string (private)
|Sanitized command or raises ValueError

|`_execute_shell(command)`
|Shell command (private)
|Tuple: (formatted, raw_data)

|`_execute_nodetool(command)`
|Nodetool command (private)
|Tuple: (formatted, parsed_data)
|===

== Output Parsing

=== parsers.py

Parses nodetool and shell command outputs into structured data with robust error handling.

==== NodetoolParser

Parses Cassandra nodetool command outputs using regex for reliability.

*Supported Commands:*
- `status` - Node status, load, and ownership (regex-based parsing)
- `compactionstats` - Compaction statistics
- `tpstats` - Thread pool statistics
- `describecluster` - Cluster topology and schema versions
- `tablestats` - Table-level statistics
- `info` - Node information summary
- `gcstats` - Garbage collection statistics
- `gossipinfo` - Gossip protocol information

*Usage:*
[source,python]
----
from plugins.common import NodetoolParser

parser = NodetoolParser()

# Parse nodetool status (now uses regex!)
status_output = """
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address       Load       Tokens  Owns    Host ID                               Rack
UN  192.168.1.10  108.45 KB  256     33.3%   aaa-bbb-ccc                          rack1
UN  192.168.1.11  1.5 GB     256     33.3%   ddd-eee-fff                          rack1
DN  192.168.1.12  0 B        256     33.3%   ggg-hhh-iii                          rack1
"""

result = parser.parse('status', status_output)
# Returns list of node dicts with proper load parsing
----

*Improvements Over Previous Version:*

[cols="1,2,2"]
|===
|Feature |Old Approach |New Approach

|Load Parsing
|String splitting (breaks with spaces)
|Regex pattern matching (robust)

|Error Handling
|Basic try/catch
|`_safe_int()`, `_safe_float()` helpers

|NaN Handling
|Could crash on NaN strings
|Properly returns None for NaN/N/A

|Code Quality
|Duplicate parsers
|Single, well-tested implementation
|===

*Helper Functions:*

[source,python]
----
from plugins.common.parsers import _parse_size_to_bytes, _safe_int, _safe_float

# Convert size strings to bytes
bytes_val = _parse_size_to_bytes("108.45 KB")  # Returns: 111052
bytes_val = _parse_size_to_bytes("1.5 GB")     # Returns: 1610612736

# Safely convert values
int_val = _safe_int("123")        # Returns: 123
int_val = _safe_int("NaN")        # Returns: None
int_val = _safe_int(None)         # Returns: 0 (default)
int_val = _safe_int("invalid")    # Returns: 0 (default)

float_val = _safe_float("123.45") # Returns: 123.45
float_val = _safe_float("NaN")    # Returns: None
----

*Output Structures:*

*nodetool status (regex-based):*
[source,python]
----
[
    {
        'datacenter': 'datacenter1',
        'status': 'U',              # U=Up, D=Down
        'state': 'N',               # N=Normal, L=Leaving, J=Joining, M=Moving
        'address': '192.168.1.10',
        'load': '108.45 KB',        # Preserved as string
        'load_bytes': 111052,       # Converted to bytes
        'tokens': 256,
        'owns_effective_percent': 33.3,
        'host_id': 'aaa-bbb-ccc',
        'rack': 'rack1'
    }
]
----

*nodetool info:*
[source,python]
----
{
    'id': 'aaa-bbb-ccc-ddd',
    'gossip_active': True,
    'native_transport_active': True,
    'load': '108.45 KB',
    'load_bytes': 111052,
    'uptime_seconds': 86400,
    'heap_memory_mb_used': 512.0,
    'heap_memory_mb_total': 2048.0,
    'heap_memory_percent': 25.0,
    'datacenter': 'datacenter1',
    'rack': 'rack1',
    'percent_repaired': 100.0
}
----

*nodetool gcstats:*
[source,python]
----
{
    'interval_ms': 1000,
    'max_gc_elapsed_ms': 50,
    'total_gc_elapsed_ms': 100,
    'stdev_gc_elapsed_ms': 10,
    'gc_reclaimed_mb': 256,
    'collections': 10,
    'direct_memory_bytes': 1073741824
}
----

*Error Handling:*
[source,python]
----
try:
    result = parser.parse('status', output)
except ValueError as e:
    logger.error(f"Parse error: {e}")
    result = []
----

==== ShellCommandParser

Parses common shell command output.

*Supported Commands:*
- `df` - Disk usage
- `free` - Memory usage

[source,python]
----
from plugins.common import ShellCommandParser

# Parse df output
df_output = """
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda1       100G   75G   25G  75% /
"""
filesystems = ShellCommandParser.parse_df(df_output)
# Returns: [{'filesystem': '/dev/sda1', 'size': '100G', 'used': '75G', ...}]

# Parse free output
free_output = """
              total        used        free      shared  buff/cache   available
Mem:           8192        4096        2048         512        2048        6144
"""
memory = ShellCommandParser.parse_free(free_output)
# Returns: {'total': '8192', 'used': '4096', 'free': '2048', ...}
----

== Output Formatting

=== output_formatters.py

Formats data into AsciiDoc markup for health check reports with proper escaping.

==== AsciiDocFormatter

Formats various data types into AsciiDoc tables and admonitions.

*Format Query Results:*
[source,python]
----
from plugins.common import AsciiDocFormatter

formatter = AsciiDocFormatter()

# Format list of dicts as table
data = [
    {'name': 'keyspace1', 'replication_factor': 3},
    {'name': 'keyspace2', 'replication_factor': 1}
]

table = formatter.format_table(data)
print(table)
----

*Output:*
[source,asciidoc]
----
|===
|name|replication_factor
|keyspace1|3
|keyspace2|1
|===
----

*Format Admonitions (NEW):*
[source,python]
----
# Available admonition types
note = formatter.format_note("Everything is healthy")
warning = formatter.format_warning("High disk usage detected")
critical = formatter.format_critical("Node is down!")
error = formatter.format_error("Query failed")
tip = formatter.format_tip("Run nodetool repair")
----

*Output:*
[source,asciidoc]
----
[NOTE]
====
Everything is healthy
====

[WARNING]
====
High disk usage detected
====

[IMPORTANT]
====
Node is down!
====

[CAUTION]
====
Query failed
====

[TIP]
====
Run nodetool repair
====
----

*Specialized Formatters (NEW):*
[source,python]
----
# Format nodetool status (simplified display)
nodes = [{'datacenter': 'dc1', 'status': 'U', 'address': '192.168.1.10', ...}]
formatted = formatter.format_nodetool_status(nodes)

# Format thread pool stats
pools = [{'pool_name': 'ReadStage', 'active': 0, 'pending': 0, ...}]
formatted = formatter.format_nodetool_tpstats(pools)

# Format compaction stats
stats = {'pending_tasks': 5, 'active_compactions': [...]}
formatted = formatter.format_nodetool_compactionstats(stats)

# Format dictionary as two-column table
data = {'setting1': 'value1', 'setting2': 'value2'}
formatted = formatter.format_dict_as_table(data, 'Setting', 'Value')
----

*AsciiDoc Escaping (NEW):*
[source,python]
----
# Properly escapes special characters
formatter._escape_asciidoc("Text with | pipes")  # Returns: "Text with \\| pipes"
formatter._escape_asciidoc("Text with [brackets]")  # Returns: "Text with \\[brackets\\]"
formatter._escape_asciidoc("Text with \\ backslash")  # Returns: "Text with \\\\ backslash"
----

*Key Methods:*

[cols="1,2,2"]
|===
|Method |Parameters |Returns

|`format_table(data)`
|List of dicts or list of lists
|AsciiDoc table string

|`format_shell_output(command, output)`
|Command name and output
|Formatted code block or table

|`format_literal(text)`
|Raw text
|AsciiDoc literal block

|`format_note(message)`
|Message string
|NOTE admonition

|`format_warning(message)`
|Message string
|WARNING admonition

|`format_critical(message)`
|Message string
|IMPORTANT admonition

|`format_error(message)`
|Message string
|CAUTION admonition

|`format_tip(message)`
|Message string
|TIP admonition

|`format_nodetool_status(nodes)`
|List of node dicts
|Formatted table

|`format_dict_as_table(data, key_header, value_header)`
|Dict and optional headers
|Two-column table
|===

== Check Helpers

=== check_helpers.py

Reusable helper functions that reduce boilerplate in health check modules.

*All functions now have complete type hints for better IDE support!*

==== require_ssh()

Checks if SSH is configured and available for the connector.

*Usage:*
[source,python]
----
from plugins.common.check_helpers import require_ssh

def run_my_check(connector, settings):
    adoc_content = []
    structured_data = {}
    
    # Check SSH availability
    ssh_ok, skip_msg, skip_data = require_ssh(connector, "nodetool commands")
    if not ssh_ok:
        adoc_content.append(skip_msg)
        structured_data["result"] = skip_data
        return "\n".join(adoc_content), structured_data
    
    # SSH is available, proceed with check
    # ...
----

==== require_aws(), require_azure(), require_instaclustr() (NEW)

Similar helpers for cloud platform checks.

*Usage:*
[source,python]
----
from plugins.common.check_helpers import require_aws, require_azure, require_instaclustr

# Check AWS availability
aws_ok, skip_msg, skip_data = require_aws(connector, "CloudWatch metrics")
if not aws_ok:
    return skip_msg, skip_data

# Check Azure availability
azure_ok, skip_msg, skip_data = require_azure(connector, "Azure Monitor metrics")
if not azure_ok:
    return skip_msg, skip_data

# Check Instaclustr availability
ic_ok, skip_msg, skip_data = require_instaclustr(connector, "cluster metrics")
if not ic_ok:
    return skip_msg, skip_data
----

==== format_check_header() (ENHANCED)

Creates standardized check headers with multiple requirement types.

*Usage:*
[source,python]
----
from plugins.common.check_helpers import format_check_header

# Check with multiple requirements
adoc_content = format_check_header(
    "CloudWatch Metrics Analysis",
    "Analyzing RDS CloudWatch metrics for performance insights.",
    requires_ssh=False,
    requires_aws=True,
    requires_azure=False,
    requires_instaclustr=False
)
----

*Output:*
[source,python]
----
[
    "=== CloudWatch Metrics Analysis",
    "",
    "Analyzing RDS CloudWatch metrics for performance insights.",
    "",
    "[NOTE]",
    "====",
    "**Requirements:**",
    "",
    "* AWS credentials with CloudWatch read permissions",
    "===="
]
----

==== safe_execute_query() (ENHANCED)

Wraps query execution with consistent error handling and supports parameterized queries.

*Usage:*
[source,python]
----
from plugins.common.check_helpers import safe_execute_query

# Simple query
success, formatted, raw = safe_execute_query(
    connector, 
    "SELECT * FROM system.peers", 
    "System peers query"
)

# Parameterized query (NEW!)
success, formatted, raw = safe_execute_query(
    connector,
    "SELECT * FROM system.peers WHERE data_center = %(dc)s",
    "Peers by datacenter query",
    params={'dc': 'datacenter1'}
)

if not success:
    # Query failed, formatted contains error message
    adoc_content.append(formatted)
    structured_data["result"] = {"status": "error", "data": raw}
    return "\n".join(adoc_content), structured_data
----

==== merge_structured_data() (NEW)

Safely merge multiple query results into structured data dictionary.

*Usage:*
[source,python]
----
from plugins.common.check_helpers import merge_structured_data

structured_data = {}

# First query
success, formatted, raw = safe_execute_query(connector, query1, "Query 1")
structured_data = merge_structured_data(
    structured_data,
    {"node_status": raw},
    "cluster_health"
)

# Second query
success, formatted, raw = safe_execute_query(connector, query2, "Query 2")
structured_data = merge_structured_data(
    structured_data,
    {"compaction_stats": raw},
    "cluster_health"
)

# Result:
# structured_data = {
#     "cluster_health": {
#         "node_status": [...],
#         "compaction_stats": {...}
#     }
# }
----

==== calculate_percentage() (NEW)

Safely calculate percentages with zero-division handling.

*Usage:*
[source,python]
----
from plugins.common.check_helpers import calculate_percentage

pct = calculate_percentage(75, 100)      # Returns: 75.0
pct = calculate_percentage(1, 3, 2)      # Returns: 33.33 (2 decimal places)
pct = calculate_percentage(5, 0)         # Returns: None (safe!)
pct = calculate_percentage(50, 200, 0)   # Returns: 25 (0 decimal places)
----

==== format_bytes() (NEW)

Convert bytes to human-readable format.

*Usage:*
[source,python]
----
from plugins.common.check_helpers import format_bytes

format_bytes(1024)               # Returns: "1.00 KB"
format_bytes(1536)               # Returns: "1.50 KB"
format_bytes(1073741824)         # Returns: "1.00 GB"
format_bytes(1536, 1)            # Returns: "1.5 KB" (1 decimal place)
format_bytes(0)                  # Returns: "0 B"
----

== Retry Utilities (NEW)

=== retry_utils.py

Provides retry logic with exponential backoff for handling transient failures.

==== retry_on_failure()

Decorator to retry function calls on failure.

*Usage:*
[source,python]
----
from plugins.common import retry_on_failure
from botocore.exceptions import ClientError

@retry_on_failure(
    max_attempts=3,
    delay=1.0,
    backoff=2.0,
    exceptions=(ClientError,),
    log_attempts=True
)
def fetch_cloudwatch_metrics():
    # API call that might fail transiently
    response = cloudwatch.get_metric_statistics(...)
    return response

# Will retry up to 3 times with exponential backoff (1s, 2s, 4s)
result = fetch_cloudwatch_metrics()
----

*Parameters:*

[cols="1,2,1"]
|===
|Parameter |Description |Default

|`max_attempts`
|Maximum number of attempts
|3

|`delay`
|Initial delay between retries (seconds)
|1.0

|`backoff`
|Multiplier for delay after each retry
|2.0

|`exceptions`
|Exception type(s) to catch and retry
|Exception

|`log_attempts`
|Whether to log retry attempts
|True
|===

==== should_retry_error()

Determine if an error should trigger a retry based on error codes.

*Usage:*
[source,python]
----
from plugins.common import should_retry_error

try:
    response = api.call()
except Exception as e:
    if should_retry_error(e):
        # Retry the operation
        time.sleep(1)
        response = api.call()
    else:
        # Don't retry, raise immediately
        raise
----

*Default Retryable Patterns:*
- Throttling
- RequestLimitExceeded
- ServiceUnavailable
- InternalError
- TooManyRequests
- HTTP 429, 500, 502, 503, 504

== Cloud Platform Integrations

=== aws_handler.py

AWS RDS and CloudWatch integration with retry logic and error classification.

==== AWSConnectionManager

*Features:*
- ‚úÖ RDS instance details retrieval
- ‚úÖ CloudWatch metrics with retry logic
- ‚úÖ Error classification (permissions, throttling, invalid params)
- ‚úÖ Automatic credential loading from config file
- ‚úÖ Configurable retry behavior

*Configuration:*
[source,yaml]
----
# config/aws_credentials.yaml
aws_region: us-east-1
aws_access_key_id: AKIAIOSFODNN7EXAMPLE
aws_secret_access_key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
----

*Usage:*
[source,python]
----
from plugins.common import AWSConnectionManager

settings = {
    'aws_region': 'us-east-1',
    'aws_access_key_id': 'YOUR_KEY',
    'aws_secret_access_key': 'YOUR_SECRET'
}

aws_manager = AWSConnectionManager(settings)

# Get RDS instance details (with retry)
instance = aws_manager.get_instance_details('my-db-instance')
# Returns: {
#     'instance_id': 'my-db-instance',
#     'instance_class': 'db.t3.medium',
#     'engine': 'postgres',
#     'engine_version': '14.7',
#     'allocated_storage_gb': 100,
#     'status': 'available',
#     'endpoint': 'my-db.region.rds.amazonaws.com',
#     'port': 5432,
#     'availability_zone': 'us-east-1a',
#     'multi_az': True
# }

# Get CloudWatch metrics (with retry and error classification)
metrics = aws_manager.get_cloudwatch_metrics(
    dimensions=[{'Name': 'DBInstanceIdentifier', 'Value': 'my-db-instance'}],
    metrics_to_fetch=[
        {'Namespace': 'AWS/RDS', 'MetricName': 'CPUUtilization', 
         'Statistic': 'Average', 'Unit': 'Percent'},
        {'Namespace': 'AWS/RDS', 'MetricName': 'DatabaseConnections',
         'Statistic': 'Average', 'Unit': 'Count'}
    ],
    hours=24,
    period=3600
)
# Returns: {
#     'CPUUtilization': {
#         'value': 45.2,
#         'unit': 'Percent',
#         'statistic': 'Average',
#         'timestamp': '2025-10-19T12:00:00Z',
#         'status': 'success'
#     },
#     'DatabaseConnections': {
#         'value': 'N/A',
#         'note': 'Rate limited',
#         'error_code': 'Throttling',
#         'status': 'rate_limited'
#     }
# }
----

*Error Status Codes:*
- `success` - Metric fetched successfully
- `no_data` - No datapoints available
- `permission_error` - Access denied or unauthorized
- `rate_limited` - API throttling or request limit exceeded
- `invalid_parameters` - Invalid parameter values
- `error` - Other errors

=== azure_handler.py

Azure PostgreSQL and Azure Monitor integration with retry logic.

==== AzureConnectionManager

*Features:*
- ‚úÖ Azure PostgreSQL server details
- ‚úÖ Azure Monitor metrics with retry logic
- ‚úÖ Error classification (403, 429, 5xx errors)
- ‚úÖ DefaultAzureCredential for flexible authentication
- ‚úÖ Automatic credential loading

*Configuration:*
[source,yaml]
----
# config/azure_credentials.yaml
subscription_id: 12345678-1234-1234-1234-123456789012
client_id: abcdef12-3456-7890-abcd-ef1234567890
client_secret: your-client-secret
tenant_id: 87654321-4321-4321-4321-210987654321
----

*Usage:*
[source,python]
----
from plugins.common import AzureConnectionManager

settings = {
    'subscription_id': '12345678-1234-1234-1234-123456789012',
    'resource_group': 'my-resource-group',
    'server_name': 'my-postgres-server'
}

azure_manager = AzureConnectionManager(settings)

# Get server details (with retry)
server = azure_manager.get_server_details()
# Returns: {
#     'server_name': 'my-postgres-server',
#     'location': 'eastus',
#     'version': '14',
#     'sku_name': 'Standard_D4s_v3',
#     'sku_tier': 'GeneralPurpose',
#     'sku_capacity': 4,
#     'storage_mb': 102400,
#     'backup_retention_days': 7,
#     'geo_redundant_backup': 'Enabled',
#     'ssl_enforcement': 'Enabled',
#     'user_visible_state': 'Ready'
# }

# Get metrics (with retry and error classification)
resource_id = f"/subscriptions/{sub_id}/resourceGroups/{rg}/providers/Microsoft.DBforPostgreSQL/servers/{server}"
metrics = azure_manager.get_metrics(resource_id, ['cpu_percent', 'active_connections'])
# Returns: {
#     'cpu_percent': {
#         'value': 35.7,
#         'unit': 'Percent',
#         'timestamp': '2025-10-19T12:00:00Z',
#         'status': 'success'
#     },
#     'active_connections': {
#         'value': 'N/A',
#         'note': 'Permission denied (403)',
#         'status': 'permission_error'
#     }
# }
----

*Error Status Codes:*
- `success` - Metric fetched successfully
- `no_data` - No datapoints or time series data
- `permission_error` - HTTP 403
- `rate_limited` - HTTP 429
- `service_error` - HTTP 500, 502, 503, 504
- `error` - Other errors

=== instaclustr_handler.py

NetApp Instaclustr API integration with retry logic.

==== InstaclustrConnectionManager

*Features:*
- ‚úÖ Cluster details from Instaclustr API
- ‚úÖ Metrics retrieval with retry logic
- ‚úÖ Error classification (401, 403, 429, 5xx)
- ‚úÖ Configurable timeout
- ‚úÖ Automatic credential loading

*Configuration:*
[source,yaml]
----
# config/instaclustr_credentials.yaml
instaclustr_api_key: your-api-key-here
instaclustr_cluster_id: 12345678-1234-1234-1234-123456789012
----

*Usage:*
[source,python]
----
from plugins.common import InstaclustrConnectionManager

settings = {
    'instaclustr_api_key': 'your-api-key',
    'instaclustr_cluster_id': '12345678-1234-1234-1234-123456789012',
    'instaclustr_timeout': 30  # Optional
}

ic_manager = InstaclustrConnectionManager(settings)

# Get cluster details (with retry)
cluster = ic_manager.get_cluster_details()
# Returns: {
#     'cluster_id': '12345678-1234-1234-1234-123456789012',
#     'name': 'My Cassandra Cluster',
#     'status': 'RUNNING',
#     'data_centres': 2,
#     'nodes': ['node-1-id', 'node-2-id', 'node-3-id'],
#     'node_count': 3
# }

# Get metrics (with retry and error classification)
metrics = ic_manager.get_metrics('health', hours=24)
# Returns: {
#     'metric_type': 'health',
#     'value': 'HEALTHY',
#     'timestamp': '2025-10-19T12:00:00Z',
#     'note': '',
#     'status': 'success'
# }
----

*Error Status Codes:*
- `success` - Metric fetched successfully
- `auth_error` - HTTP 401 (invalid API key)
- `permission_error` - HTTP 403
- `rate_limited` - HTTP 429
- `service_error` - HTTP 500, 502, 503, 504
- `timeout` - Request timeout
- `error` - Other errors

== Best Practices

=== SSH Connection Management

*Do:*
[source,python]
----
# ‚úÖ Use context manager for automatic cleanup
with SSHConnectionManager(settings) as ssh:
    stdout, stderr, exit_code = ssh.execute_command('df -h')

# ‚úÖ Enable host key checking in production
settings['ssh_strict_host_key_checking'] = True

# ‚úÖ Use ensure_connected() for long-running operations
ssh_manager.ensure_connected()
result = ssh_manager.execute_command('nodetool repair')
----

*Don't:*
[source,python]
----
# ‚ùå Don't leave connections open
ssh = SSHConnectionManager(settings)
ssh.connect()
# ... forgot to disconnect()

# ‚ùå Don't disable host key checking in production
settings['ssh_strict_host_key_checking'] = False  # INSECURE!

# ‚ùå Don't ignore connection errors
ssh.connect()  # No error handling
----

=== Command Execution Security

*Do:*
[source,python]
----
# ‚úÖ Use default secure mode
executor = ShellExecutor(ssh_manager)

# ‚úÖ Only disable sanitization if absolutely necessary and in trusted environments
if in_trusted_environment():
    executor = ShellExecutor(ssh_manager, allow_unsafe_commands=True)
----

*Don't:*
[source,python]
----
# ‚ùå Don't blindly disable sanitization
executor = ShellExecutor(ssh_manager, allow_unsafe_commands=True)  # DANGEROUS!

# ‚ùå Don't construct commands from untrusted user input
user_input = request.get('command')
executor.execute(f'{{"operation": "shell", "command": "{user_input}"}}')  # INJECTION RISK!
----

=== Parser Usage

*Do:*
[source,python]
----
# ‚úÖ Always handle parse errors
try:
    result = parser.parse('status', output)
except ValueError as e:
    logger.error(f"Parse error: {e}")
    result = []

# ‚úÖ Use helper functions for safety
value = _safe_int("123")
size_bytes = _parse_size_to_bytes("1.5 GB")
----

*Don't:*
[source,python]
----
# ‚ùå Don't assume parsing always succeeds
result = parser.parse('status', output)
first_node = result[0]  # May fail if result is empty

# ‚ùå Don't manually parse complex output
load_parts = load_str.split()  # Fragile!
load_value = float(load_parts[0])  # May fail on "N/A" or malformed data
----

=== Cloud API Usage

*Do:*
[source,python]
----
# ‚úÖ Check status field in responses
metrics = aws_manager.get_cloudwatch_metrics(...)
for metric_name, data in metrics.items():
    if data['status'] == 'success':
        value = data['value']
        # Process value
    elif data['status'] == 'rate_limited':
        logger.warning(f"Rate limited for {metric_name}, retry later")
    elif data['status'] == 'permission_error':
        logger.error(f"Missing permissions for {metric_name}")

# ‚úÖ Let retry decorator handle transient failures automatically
@retry_on_failure(max_attempts=3, delay=1)
def fetch_data():
    return api.get_data()
----

*Don't:*
[source,python]
----
# ‚ùå Don't assume all metrics succeed
metrics = aws_manager.get_cloudwatch_metrics(...)
cpu_value = metrics['CPUUtilization']['value']  # May be 'N/A'!

# ‚ùå Don't ignore error classification
if metrics['CPUUtilization']['value'] == 'N/A':
    # Generic handling - doesn't distinguish rate limit from permissions error
    pass
----

== Migration Guide

=== From Manual SSH to SSHConnectionManager

*Before:*
[source,python]
----
import paramiko

ssh_client = paramiko.SSHClient()
ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())  # INSECURE!
ssh_client.connect(
    hostname=settings['ssh_host'],
    username=settings['ssh_user'],
    key_filename=settings['ssh_key_file']
)

stdin, stdout, stderr = ssh_client.exec_command('df -h')
output = stdout.read().decode()

ssh_client.close()
----

*After:*
[source,python]
----
from plugins.common import SSHConnectionManager

settings['ssh_strict_host_key_checking'] = True  # Secure by default

with SSHConnectionManager(settings) as ssh:
    stdout, stderr, exit_code = ssh.execute_command('df -h')
    # output is already decoded and validated
----

=== From Manual Parsing to NodetoolParser

*Before:*
[source,python]
----
output = execute_nodetool('status')
lines = output.split('\n')
nodes = []

for line in lines:
    if line.startswith('UN') or line.startswith('DN'):
        parts = line.split()  # Breaks with "108.45 KB"
        nodes.append({
            'status': parts[0][0],
            'address': parts[1],
            'load': parts[2] + ' ' + parts[3],  # Manual parsing
            # ... manual parsing
        })
----

*After:*
[source,python]
----
from plugins.common import NodetoolParser

parser = NodetoolParser()
output = execute_nodetool('status')
nodes = parser.parse('status', output)  # Fully parsed with regex
# Each node has 'load' as string and 'load_bytes' as int
----

=== Adding Cloud Support to Existing Connector

*Before (No cloud support):*
[source,python]
----
class PostgreSQLConnector:
    def __init__(self, settings):
        self.settings = settings
        self.client = psycopg2.connect(...)
----

*After (With AWS support):*
[source,python]
----
from plugins.common import SSHSupportMixin, AWSSupportMixin, AWSConnectionManager

class PostgreSQLConnector(SSHSupportMixin, AWSSupportMixin):
    def __init__(self, settings):
        self.settings = settings
        self.client = psycopg2.connect(...)
        
        # Add AWS support
        if settings.get('aws_region'):
            self.aws_manager = AWSConnectionManager(settings)
        else:
            self.aws_manager = None
    
    def get_cloudwatch_metrics(self, ...):
        if not self.has_aws_support():
            return self.get_aws_skip_message("CloudWatch metrics")
        
        return self.aws_manager.get_cloudwatch_metrics(...)
----

== Troubleshooting

=== SSH Connection Failures

[cols="1,2,2"]
|===
|Problem |Possible Cause |Solution

|Connection refused
|SSH port blocked
|Check firewall rules, verify SSH is running

|Host key verification failed
|Host key changed or not in known_hosts
|Add host to known_hosts or use `ssh_strict_host_key_checking=False` (dev only!)

|Authentication failed
|Wrong key or permissions
|Verify key file permissions (chmod 600), check username

|Connection timeout
|Network issue or wrong host
|Verify host is reachable, check network connectivity
|===

=== Command Sanitization Errors

[cols="1,2,2"]
|===
|Problem |Possible Cause |Solution

|Command rejected with dangerous pattern
|Command contains `;`, `&&`, etc.
|Add command to SAFE_COMMANDS or use `allow_unsafe_commands=True`

|Safe command blocked
|Command not in whitelist
|The sanitizer logs the command - review and add to SAFE_COMMANDS if legitimate

|Need to run complex shell script
|Multiple commands required
|Put commands in a script file on the server, execute script instead
|===

=== Parser Failures

[cols="1,2,2"]
|===
|Problem |Possible Cause |Solution

|Parse returns empty
|Output format changed
|Check Cassandra version, update parser if needed

|KeyError during parsing
|Unexpected output structure
|Enable debug logging to see raw output

|Load parsing fails
|Unusual size format
|Check `_parse_size_to_bytes()` supports the unit
|===

=== Cloud API Errors

[cols="1,2,2"]
|===
|Problem |Possible Cause |Solution

|Permission denied (403)
|Missing IAM permissions
|Add required permissions to IAM role/user

|Rate limited (429)
|Too many requests
|Check status field, implement exponential backoff (automatic with retry decorator)

|Service error (5xx)
|AWS/Azure service issue
|Wait and retry (automatic), check service health status

|Timeout
|Slow API or network
|Increase timeout setting
|===

== Version Compatibility

*Python:*
- ‚úÖ Tested with Python 3.8+
- ‚úÖ Fully compatible with Python 3.10+
- ‚úÖ Type hints require Python 3.7+

*Dependencies:*
- paramiko 2.x+ (SSH connections)
- boto3 1.x+ (AWS support)
- azure-identity, azure-monitor-query, azure-mgmt-rdbms (Azure support)
- requests 2.x+ (Instaclustr support)
- pyyaml 5.x+ (Configuration loading)

*Database Versions:*
- ‚úÖ Cassandra 3.x and 4.x supported
- ‚ö†Ô∏è Some nodetool output formats differ between versions
- üí° Use version detection in connectors for compatibility

== Contributing

When adding new utilities to `plugins/common`:

1. **Follow existing patterns** - Look at similar utilities
2. **Add comprehensive docstrings** - Include usage examples and type hints
3. **Write unit tests** - Test all public methods
4. **Update this README** - Document new utilities with examples
5. **Export in __init__.py** - Make utilities easily accessible
6. **Add security considerations** - Document any security implications
7. **Include error handling** - Use try/except with proper logging
8. **Consider retry logic** - Use `@retry_on_failure` for API calls

== Additional Resources

*Related Documentation:*
- Plugin-specific READMEs (e.g., `plugins/cassandra/README.adoc`)
- Main framework documentation (`README.adoc`)
- Check generation prompts (`tools/templates/check_generation/`)

*External References:*
- http://www.paramiko.org/[Paramiko Documentation]
- https://boto3.amazonaws.com/v1/documentation/api/latest/index.html[AWS Boto3 Documentation]
- https://docs.microsoft.com/en-us/python/api/overview/azure/[Azure SDK for Python]
- https://www.instaclustr.com/support/api-integrations/[Instaclustr API Documentation]
- https://docs.python.org/3/library/logging.html[Python Logging]
- https://asciidoc.org/[AsciiDoc Documentation]

== License

This module is part of the pg_healthcheck2 framework.
